\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage[noend]{algpseudocode}
\usepackage{amsmath}
\usepackage{graphicx}
%\usepackage{cleveref}
\usepackage{wrapfig}
\usepackage{bbm} % for indicator function \mathbbm{1}
\usepackage{pdfpages}
%\usepackage{multirow}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{geometry}
\usepackage{float}
\newcommand{\loss}{\mathcal{L}}

%\usepackage{matlab-prettifier}
%\lstset{language=Matlab,
%numbers=left,
%numberstyle={\tiny \color{black}},% size of the numbers
%numbersep=9pt, % this defines how far the numbers are from the text
%}

\geometry{margin=2.5cm, tmargin=2.5cm}
\author{Candidate Number: 5590E}
\title{Technical Milestone Report}

\date{\today}
\begin{document}

%\includepdf[pages=-]{coversheet.pdf}

\maketitle

\renewcommand{\abstractname}{Summary}
\begin{abstract}
Technical Summary
\end{abstract}


\section{Introduction}

AlphaGo Zero was a revolutionary AI that used reinforcement learning from self play to achieve superhuman performance in the game of go, winning 100â€“0 against the previously published, champion-defeating AlphaGo. AlphaZero is a more general algorithm that can achieve superhuman performance in many games, however it uses many of the same algorithms as AlphaGo Zero.

One of the major benefits of AlphaZero is that it only requires to be able to simulate the environment. It does not need to be told how to win, or to be given an exact model of the system dynamics, $p(s_{t+1}, return_{t+1} |s_t, a_t)$, as this can be learnt through self-play. Amazingly,this self play often leads to novel solutions to problems, as shown by 'move 37' in a game of Go against the reigning world champion, Lee Sedol.

Its aim is to learn a policy, $p(\boldsymbol{a}|\boldsymbol{s})$ (a $p.m.f.$ over actions given a state, where the probabilities are proportional to the expected outcome for each action). It achieves this by playing against itself - picking the moves that it thinks will do best, whilst inconveniencing the opponent the most (which is itself). Randomness in the moves picked gives the variability within games and allows it to actually win. Two key algorithms have been pioneered to achieve this: A convolutional neural network (cnn) and 'Monte-Carlo Tree Search' (MCTS).

AlphaGo Zero and AlphaZero use two different methods of policy improvement. For both, the weights and biases are randomly initialized (so there should be a 50\% chance of winning). Then a batch of games/episodes are played via self-play. With AlphaGo Zero, the self-play games are generated by the best player from all previous iterations and then the new player was compared with the best player; whereas AlphaZero just maintains a single neural network that is updated continually: self-play games are generated by using the latest parameters of the neural network. \cite{AlphaZero}

\begin{wrapfigure}{l}{0.6\textwidth}
   \centering
   \includegraphics[width=0.55\textwidth]{Figures/SelfPlay.jpg}
   \caption{A schematic showing how self-play and policy training are performed. Taken from \cite{AlphaGoZero}.}
   \label{fig:selfplay} 
\end{wrapfigure}

Figure \ref{fig:selfplay}a
shows how self-play is performed in AlphaGo Zero. An episode/game, $\{s_1, ..., s_T\}$ is played against itself. For each state, $s_t$, a Monte-Carlo Tree Search is performed, guided by the current policy $f_\theta$. The MCTS outputs an improved action-probability, $\boldsymbol{\pi}_t = [\hat{p}(a_1|s_t), \hat{p}(a_2|s_t), ..., \hat{p}(a_N|s_t)]$. The next move is then selected by sampling from $\boldsymbol{\pi}_t$. The final player of the game is then given a score of 1 or -1 if they win or lose respectively. The game score, z, is then appended to each state-action pair depending on which who the current player was on that move $[s_t, \boldsymbol{\pi}_t, z]$. 

Figure \ref{fig:selfplay}b
depicts how the policy is trained. The state, $s_t$, is taken as input and is passed through a neural network with parameters $\theta$. The network outputs an action-probability, $\boldsymbol{p}_t$, and a state value - the estimated probability of the current player winning the game. The neural network is trained to more closely match the MCTS-informed action-probabilities $\pi_t$, and the predicted winner (state-value), $v_t$ to the actual winner, z.

The loss for the neural network is given by:

\begin{equation}
   \loss = (z - v)^2 - \boldsymbol{\pi} \cdot log(\boldsymbol{p}) + c||\theta||^2
   \label{eqn:loss}
\end{equation}

Figure \ref{fig:MCTS}a:
A MCTS is performed at each step of an episode. The state at which the tree search starts then becomes the root state, $s_{root}$. From the root state, the tree search can move to an edge (s, a) by selecting an action, a. Each edge stores a prior action-probability output by the policy, p(s, a); a visit count, N(s, a); and an action-value, Q(s, a), which is the value of the state that action a will result in. Actions are selected by maximising an the action-value plus an upper confidence bound:

\begin{equation}
   a_{selected} = \underset{\forall a}{argmax} \Big{\{}Q(s, a) + c \cdot \frac{p(s, a)}{1 + N(s, a)}\Big{\}}
   \label{eqn:ucb}
\end{equation}

Where c is a constant of magnitude $\sim 1$.

Figure \ref{fig:MCTS}b:
Once a leaf node (N(s, a) = 0) is reached, the neural network is evaluated at that state: $f_\theta (s) = (p(s, \cdot), V(s))$. The action-probability and state-value are stored for the leaf state.

Figure \ref{fig:MCTS}c:
The action-values, Q, are calculated as the mean of state-values in the subtree below that action. The state-value are then calculated as the mean of all the action-values branching from that state, and so on.

\begin{equation}
   Q(s, a) = \frac{1}{N(s, a)} \sum_{s_{t+1} | s_t, a_t} v(s_{t+1})
   \label{eqn:actionvalue}
\end{equation}

Figure \ref{fig:MCTS}d:
After a set number searches, the MCTS-improved action-probabilities $\boldsymbol{\pi} = p(\boldsymbol{a} | s_{root})$ are returned, $\propto N^{1/\tau}$, where N is the visit count of each move from the root state and $\tau$ controls the sparseness of the probability mass function $(\{\tau = 0\} \rightarrow argmax\{N(s, a)\})$.

\begin{figure}[H]
   \centering
   \includegraphics[width=\textwidth]{Figures/MCTS.jpg}
   \caption{\label{fig:MCTS} A schematic outlining the steps involved in a monte-carlo tree search. The figure is taken from \cite{AlphaGoZero}.}
\end{figure}

Control theory is the study of how to get optimal corrective behaviour for dynamical systems. Reinforcement learning has recently found success in control problems such as atari-games and robotics \cite{RLoverview}. A common theme with control problems is that they are easy to simulate, but hard to find the dynamics for. AlphaZero is an incredibly powerful RL algorithm that requires simulation but does not know the dynamics: therefore, it is a good candidate for a general purpose controller.

There are a few problems in adapting AlphaZero to a control problem, namely:
\begin{itemize}
   \item[-] How do we represent a continuous state as a 2D state?
   \item[-] How do we change the value function to reflect a non-stationary and continuous problem? We cannot use whether we win or lose as the value for the end of the game.
   \item[-] How do we implement self-play for problems that already have non-intelligent adversaries such as gravity?
\end{itemize}

The purpose of this project is to try to find an answer to these three questions and therefore show that AlphaZero can be used as a powerful general-purpose control algorithm.

\section{Method}

Currently, a 2D state representation and a value function have been chosen and implemented, an adversary other than gravity has not been implemented yet.

\subsection{OpenAI's Gym Environment}

OpenAI's gym is a toolkit for developing reinforcement learning algorithms. They provide 'environments' with common interfaces to allow the writing of general algorithms. Initially, the CartPole environment is being used as it is the simplest classical control problem, see figure \ref{fig:cartpole}.

\begin{figure}[H]
   \centering
   \includegraphics[width=0.9\textwidth]{Figures/Cartpole.PNG}
   \caption{\label{fig:cartpole} The OpenAI gym CartPole environment. The classical state representation is shown in the top left. Actions by the player and the adversary are taken as an impulse to the left or right.}
\end{figure}

The cart is defined as having fallen over, or 'done', if the angle from upright exceeds $12^o (\theta_{max})$ or the cart wanders $> 2.4 (x_{max})$ units from the origin. We also define steps$\_$beyonds$\_$done as the number of steps that we continue to calculate states for after it has fallen over. For each step/impulse, the 2D state is calculated and a state$\_$loss is calculated as:

\begin{equation}
   \loss = - \frac{1}{2} \bigg[ \Big(\frac{x_t}{x_{max}}\Big)^2 + \Big(\frac{\theta_t}{\theta_{max}}\Big)^2 \bigg]
\end{equation}

Thus ensuring that $0 \geq \loss \geq -1$.

\subsection{2D State Representation}

\begin{wrapfigure}{r}{0.35\textwidth}
   \vspace{-1cm}
   \centering
   \includegraphics[width=0.34\textwidth]{Figures/State2D.PNG}
   \caption{An example of a 2D state representation where there are 20 bins and 17 random actions have been taken.}
   \label{fig:state2D}
\end{wrapfigure}

The state is a histogrammed and discounted function of previous state positions and angles, i.e. $New State = Binned \; Current \; Position + \gamma * Previous State$, where $\gamma$ is a discounting factor, currently set at 0.7. The numpy library in python has a function histogram2d which allows the binning of two-dimensional arrays.

A 2D representation like this allows us to use a convolutional neural network,which has the benefit of various transformation invariances - these are particularly useful for cartPole since it is highly symmetric.

\subsection{The Value Function}

A value function is computed after an episode has completed as the discounted future losses at each state with the constraint that $\gamma^{(n-1)} < \frac{1}{20}$, where $\frac{1}{20}$ was chosen as it is a standard factor for insignificance. Since steps$\_$beyonds$\_$done (n) must be defined in the CartPoleWrapper class, this is a constant, and therefore $\gamma$ is calculated as $\gamma < \frac{1}{20}^{\frac{1}{(n-1)}}$. Algorithm \ref{alg:values} shows the procedure for calculating this. The discounted value can be written as:

\begin{align}
   v_t = \frac{\sum_{t'=t, \tau=0}^{t+k, k} \gamma^\tau \loss_{t'} }{\sum_{\tau}^k \gamma_\tau}, \hspace{2cm} \text{where } \gamma^k < \frac{1}{20}
\end{align}

\begin{algorithm}[h]
\caption{Calculate State Values}
\label{alg:values}
\begin{algorithmic}[1]
   \Procedure{value$\_$function} {state$\_$losses}
   \State {\textbf{Constants:} $\gamma$, k}\Comment{discount factor, steps$\_$beyonds$\_$done}
   \State {\textbf{Initialise:} values $\gets$ [ ]}
   \For{step$\_$idx $:=$ 0 \textbf{to} (length(state$\_$losses) - k)}
      \State state$\_$value $\gets 0$ 
      \State discount $\gets 1$
      \For{idx $:=$ step$\_$idx+1 \textbf{to} (step$\_$idx+k)}
         \State value $\gets$ value + discount$\times$state$\_$losses[idx]
         \State discount $\gets$ $\gamma \times $discount
      \EndFor
      \State values.append($\frac{\text{value}}{\sum \gamma}$)
   \EndFor
   \State \textbf{return} values \Comment{values is k elements shorter than state$\_$losses}
   \EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Program Structure}

An outline of the program structure is shown in figure \ref{fig:flowchart}. The green boxes are implemented as described in \cite{AlphaGoZero} in the introduction with the exceptions that the neural network predicts the value as defined above, not the expected outcome, and episodes are currently compared simply as $mean(\text{challenger losses}) > 0.95*mean(\text{currrent losses})$.

\section{Results}
\begin{itemize}
   \item[-] Graphs of statistics for each policy iteration, eg, errors over time for each nnet
   \item[-] Performance against random and a greedy algorithm (make the moves that move x pos closer to 0)
   \item[-] Graphs of steps taken vs expected steps left
   \item[-] Graphs of return vs time
   \item[-] If I have time, ill compare different adversaries: vs greedy, vs itself with 1/2 power etc
\end{itemize}  

\section{Discussion}
\begin{itemize}
   \item[-] Explain the graphs above
   \item[-] Talk about problems?
   \item[-] Which adversary was the best
   \item[-] Which method learnt the quickest
   \item[-] Talk about which representation of 2D state is the best? I havent done any of the others...
\end{itemize}

\section{Future Plan}

The first task to complete moving forwards is to implement an intelligent adversary. For the CartPole problem, it canâ€™t be an equal adversary since we are already fighting against gravity. Some possible adversaries are: 
\begin{itemize}
   \item[-] Alternate steps and only give the adversary 0.5F for each push
   \item[-] The adversary only plays every 3 steps
   \item[-] A more 'environmental' adversary such as changing gravity or damping 
\end{itemize}
 As a starting point, the adversary will act in the same plane as the force and act ever n steps (not necessarily 3 since this may be too strong). Later it would be interesting to see 

\begin{itemize}
   \item[-] Expansion to other problems
   \item[-] Timeline moving forward
   \item[-] Improvement of Nnet? Improvement of binning?
\end{itemize}

\begin{figure}[h]
   \centering
   \includegraphics[width=1\textwidth]{Figures/gantt.PNG}
   \caption{\label{fig:gantt} A Gantt Chart of the proposed timeline for lent and easter term.}
\end{figure}


\bibliographystyle{abbrv}
\bibliography{Report}

\begin{figure}[b]
   \centering
   \includegraphics[width=0.9\textwidth]{Figures/CartpoleFlowchart.png}
   \caption{\label{fig:flowchart} A flowchart showing the control flow of the current program. Green boxes depict more complex processes with further explanation in the main text.}
\end{figure}

\end{document}
