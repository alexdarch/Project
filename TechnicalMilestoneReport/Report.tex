\documentclass[11.7pt]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage[noend]{algpseudocode}
\usepackage{amsmath}
\usepackage{graphicx}
%\usepackage{cleveref}
\usepackage{wrapfig}
\usepackage{bbm} % for indicator function \mathbbm{1}
\usepackage{pdfpages}
%\usepackage{multirow}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[a4paper, margin=2.5cm, tmargin=2.5cm]{geometry}
\usepackage{float}
\newcommand{\loss}{\mathcal{L}}
%\usepackage{matlab-prettifier}
%\lstset{language=Matlab,
%numbers=left,
%numberstyle={\tiny \color{black}},% size of the numbers
%numbersep=9pt, % this defines how far the numbers are from the text
%}


\author{Alex Darch \\
   \textit{Supervisor:} Dr Glenn Vinnicombe}

\title{Technical Milestone Report}

\date{\today}
\begin{document}

%\includepdf[pages=-]{coversheet.pdf}

\maketitle

\renewcommand{\abstractname}{Summary}
\begin{abstract}
   This report summarises the application of the AlphaZero algorithm to control problems. AlphaZero is an extremely powerful reinforcement learning algorithm that has been used on board games. The adaptation of this discrete, win/lose paradigm requires reworking of some of the key algorithms, or conversion of control problems to a board game like setup. These changes, namely 2D state-space representation, a novel value function, policy iteration and self-play are discussed in this report in terms of progress to date and future plans.
\end{abstract}

\section{Introduction}

AlphaGo Zero is a revolutionary Reinforcement Learning algorithm that achieved superhuman performance in the game of go, winning 100â€“0 against the previously published, champion-defeating AlphaGo. It's successor, AlphaZero, is a generalised version that can achieve superhuman performance in many games. There are three key sub-algorithms that form the basis of their success: self-play, a convolutional neural network (CNN) and a Monte-Carlo tree search (MCTS). 

The aim of both is to learn a policy (train a CNN) that can accurately predict an action-probability and a value function. The action-probability, $p(\boldsymbol{a}|\boldsymbol{s})$, is a $p.m.f.$ over the valid actions given a state, where the probabilities are proportional to the expected outcome for each action, and the value function is the expected outcome of the game (e.g. +1 is a win with 100\% certainty). It achieves through self-play: picking the moves that it thinks will do best, whilst inconveniencing the opponent the most (which is itself). Moves/actions are sampled from a MCTS-informed action-probability, which provides a probabilistic balance between exploration and exploitation. 

A key feature of AlphaZero is that it only requires the ability to simulate the environment. It does not need to be told how to win, nor does it need an exact model of the system dynamics, $p(s_{t+1}, return_{t+1} |s_t, a_t)$, as this can be learnt through self-play. Furthermore, the algorithm often 'discovers' novel solutions to problems, as shown by 'move 37' in a game of Go against the reigning world champion, Lee Sedol.

AlphaZero differs from AlphaGo Zero, other than not taking advantage of the symmetries of go, by using a different method of policy improvement. For both, the weights and biases are randomly initialized (so there should be a 50\% chance of winning). Then a batch of games/episodes are played via self-play. With AlphaGo Zero, the self-play games are generated by the best player from all previous iterations and then the new player was compared with the best player; whereas AlphaZero just maintains a single neural network that is updated continually: self-play games are generated by using the latest parameters of the neural network. \cite{AlphaZero}

Figure \ref{fig:selfplay}a 
shows how self-play is performed in AlphaGo Zero. An episode/game, $\{s_1, ..., s_T\}$ is played against itself. For each state, $s_t$, a Monte-Carlo Tree Search is performed, guided by the current policy $f_\theta$. The MCTS outputs an improved action-probability, $\boldsymbol{\pi}_t = [\hat{p}(a_1|s_t), \hat{p}(a_2|s_t), ..., \hat{p}(a_N|s_t)]$. The next move is then selected by sampling from $\boldsymbol{\pi}_t$. The final player of the game is then given a score of +1 or -1 if they win or lose respectively. The game score, z, is then appended to each state-action pair depending on who the current player was on that move to give: $[s_t, \boldsymbol{\pi}_t, z]$. 

Figure \ref{fig:selfplay}b
depicts how the policy is trained. The state, $s_t$, is taken as input and is passed through a neural network with parameters $\theta$. The network outputs an action-probability, $\boldsymbol{p}_t$, and a state value - the estimated probability of the current player winning the game. The neural network is trained to more closely match the MCTS-informed action-probabilities $\boldsymbol{\pi}_t$, and the predicted winner (state-value), $v_t$ to the actual winner, z.

\begin{wrapfigure}{l}{0.6\textwidth}
   \centering
   \includegraphics[width=0.58\textwidth]{Figures/SelfPlay.jpg}
   \caption{A schematic showing how self-play and policy training are performed. Taken from \cite{AlphaGoZero}.}
   \label{fig:selfplay}
   \vspace{0.5cm}
\end{wrapfigure}

The loss function for the neural network is given by:

\begin{equation}
   \loss = (z - v)^2 - \boldsymbol{\pi} \cdot log(\boldsymbol{p}) + c||\theta||^2
   \label{eqn:loss}
\end{equation}

Figure \ref{fig:MCTS}a:
A MCTS is performed at each step of an episode. The state at which the tree search starts then becomes the root state, $s_{root}$. From the root state, the tree search can move to an edge (s, a) by selecting an action, a. Each edge stores a prior action-probability output by the policy, p(s, a); a visit count, N(s, a); and an action-value, Q(s, a), which is the value of the state that action a will result in. Actions are selected by maximising an the action-value plus an upper confidence bound: \\ \\

\begin{equation}
   a_{selected} = \underset{\forall a}{argmax} \Big{\{}Q(s, a) + c \cdot \frac{p(s, a)}{1 + N(s, a)}\Big{\}}
   \label{eqn:ucb}
\end{equation}

Where c is a constant of magnitude $\sim 1$.

Figure \ref{fig:MCTS}b:
Once a leaf node (N(s, a) = 0) is reached, the neural network is evaluated at that state: $f_\theta (s) = (p(s, \cdot), V(s))$. The action-probability and state-value are stored for the leaf state.

Figure \ref{fig:MCTS}c:
The action-values, Q, are calculated as the mean of state-values in the subtree below that action. The state-value are then calculated as the mean of all the action-values branching from that state, and so on.

\begin{equation}
   Q(s, a) = \frac{1}{N(s, a)} \sum_{s_{t+1} | s_t, a_t} v(s_{t+1})
   \label{eqn:actionvalue}
\end{equation} 

Figure \ref{fig:MCTS}d:
After a set number of searches, the MCTS-improved action-probabilities $\boldsymbol{\pi} = p(\boldsymbol{a} | s_{root})$ are returned, $\propto N^{1/\tau}$, where N is the visit count of each move from the root state and $\tau$ controls the sparseness of the probability mass function $(\{\tau = 0\} \rightarrow argmax\{N(s, a)\})$.

\begin{figure}[H]
   \centering
   \includegraphics[width=\textwidth]{Figures/MCTS.jpg}
   \caption{\label{fig:MCTS} A schematic outlining the steps involved in a monte-carlo tree search. The figure is taken from \cite{AlphaGoZero}.}
\end{figure}

Control theory is the study of how to get optimal corrective behaviour for dynamical systems. Reinforcement learning has recently found success in control problems such as Atari-games and robotics \cite{RLoverview}. A common theme with control problems is that they are easy to simulate, but hard to find the dynamics for. Therefore, adapting AlphaZero to control problems is an exciting extension to an already powerful AI.

There are a few problems in adapting AlphaZero to a control problem, namely:
\begin{itemize}
   \item[-] How do we represent a continuous state as a 2D state?
   \item[-] How do we change the value function to reflect a non-stationary and continuous problem? We cannot use whether we win or lose as the value for the end of the game.
   \item[-] How do we implement self-play for problems that already have non-intelligent adversaries such as gravity?
   \item[-] How do we compare policies when one policy has an advantage such as gravity? 
\end{itemize}

The purpose of this project is to try to find an answer to these three questions and therefore show that AlphaZero can be used as a powerful general-purpose control algorithm.

\section{Method}

Currently, a 2D state representation and a value function have been chosen and implemented, an adversary other than gravity has not been implemented yet.

\subsection{OpenAI's Gym Environment}

OpenAI's gym is a toolkit for developing reinforcement learning algorithms. They provide environments with common interfaces to allow the writing of general algorithms. Initially, the CartPole environment is being used as it is the simplest classical control problem, see figure \ref{fig:cartpole}.

\begin{figure}[H]
   \centering
   \includegraphics[width=0.7\textwidth]{Figures/Cartpole.PNG}
   \caption{\label{fig:cartpole} The OpenAI gym CartPole environment. The classical state representation is shown in the top left. Actions by the player and the adversary are taken as an impulse to the left or right.}
\end{figure}

The cart is defined as having fallen over, or 'done', if the angle from upright exceeds $12^o (\theta_{max})$ or the cart wanders $> 2.4 (x_{max})$ units from the origin. We also define steps$\_$beyonds$\_$done as the number of steps that we continue to calculate states for after it has fallen over. For each step/impulse, the 2D state is calculated and a state$\_$loss is calculated as:

\begin{equation}
   \loss = - \frac{1}{2} \bigg[ \Big(\frac{x_t}{x_{max}}\Big)^2 + \Big(\frac{\theta_t}{\theta_{max}}\Big)^2 \bigg]
\end{equation}

Thus ensuring that $0 \geq \loss \geq -1$.

\subsection{2D State Representation}

\begin{wrapfigure}{r}{0.3\textwidth}
   \vspace{-1cm}
   \centering
   \includegraphics[width=0.28\textwidth]{Figures/State2D.PNG}
   \caption{An example of a 2D state representation where there are 20 bins and 17 random actions have been taken.}
   \label{fig:state2D}
\end{wrapfigure}

The state is a histogrammed and discounted function of previous state positions and angles, i.e. $New State = Binned \; Current \; Position + \gamma * Previous State$, where $\gamma$ is a discounting factor, currently set at 0.7. The numpy library in python has a function histogram2d which allows the binning of two-dimensional arrays.

A 2D representation like this allows us to use a convolutional neural network,which has the benefit of various transformation invariances - these are particularly useful for CartPole since it is highly symmetric.

\subsection{The Value Function}

A value function is computed after an episode has completed as the discounted future losses at each state with the constraint that $\gamma^{k} < \frac{1}{20}$, where $\frac{1}{20}$ was chosen as it is a standard factor for insignificance. Since steps$\_$beyonds$\_$done (= k) must be defined in the CartPoleWrapper class, this is a constant, and therefore $\gamma$ is calculated as $\gamma < \frac{1}{20}^{\frac{1}{k}}$. The discounted values are calculated using a geometric series:

\begin{align}
   v_t = \frac{\sum_{t'=t, \tau=0}^{t+k, k} \gamma^\tau \loss_{t'} }{\sum_{\tau}^k \gamma_\tau}, \hspace{2cm} \text{where } \gamma^k < \frac{1}{20}
\end{align}

\subsection{Program Structure}

An outline of the program structure is shown in figure \ref{fig:flowchart}. The green boxes are implemented as described in \cite{AlphaGoZero} in the introduction with the exceptions that the neural network predicts the value as defined above, not the expected outcome, and episodes are currently compared simply as $mean(\text{challenger losses}) > 0.95*mean(\text{currrent losses})$.

\section{Discussion and Future Work}

A major decision with using AlphaZero for control problems is how to bin the state both for the neural network and for the Monte-Carlo tree search, with the latter being more important. For the neural network, because we are using a convolutional neural network, the output is largely invariant to the bin size. Whereas for the MCTS if the bins are too large then there is not enough precision so as to differentiate between different states, and if the bins are too small then no two states will be the same - hence the state counts, N(s), will all be 1 and the MCTS-improved action probability will effectively be uniform. Currently, the 2D state is a $20\times20$ grid and the MCTS state is calculated by multiplying the states by 100, rounding and converting to integers e.g. [-1, -3, 0, -2]. Using these, an average episode length of 148.2 was achieved on the first iteration, meaning that the MCTS is working very well (the average episode length is 7 for random moves), however, the neural network does not reduce the total loss as much as is necessary to state that the network has trained. 

One solution to the problem of deciding what size bins to use is to not use uniform binning (for both the state in the MCTS and the 2D representation for the neural net). It is more desirable to know the position closer to the zero point: if the pole is almost over then the move it right it is more obvious. Therefore a binning system that has smaller bin widths towards the zero point, e.g. using a normal distribution to represent bin density could be used.

The first novel task to complete moving forwards is to implement an intelligent adversary. For the CartPole problem, it canâ€™t be an equal adversary since we are already fighting against gravity. Some possible adversaries are: 

\begin{itemize}
   \item[-] Alternate steps and only give the adversary 0.5F for each push.
   \item[-] The adversary only plays every 3 steps.
   \item[-] A more 'environmental' adversary such as changing gravity or damping.
\end{itemize}

 As a starting point, the adversary will act in the same plane as the force and act ever n steps (not necessarily 3 since this may be too strong). The plan for future work is detailed in figure \ref{fig:gantt}

 One of the difficulties of self-play with a control problem is determining how 'good' the policy is compared to other policies. Currently, the AlphaGo Zero style of policy iteration is being used (i.e. comparing a challenger policy with the current best), however, by adopting the AlphaZero strategy of continually updating a single neural network it is difficult to know how to make the adversary roughly equal when it has the advantage of gravity. One possibility is to use a handicapped adversary and measure the performance of itself against itself. However, if the CartPole stays up for longer, is the neural network then better because it is working to keep it up better, or worse because the adversary is less effective? Further thought on this is needed, and moving forward an AlphaGo Zero style of policy iteration will be used.

\begin{figure}[h]
   \centering
   \includegraphics[width=1\textwidth]{Figures/gantt.PNG}
   \caption{\label{fig:gantt} A Gantt Chart of the proposed timeline for lent and easter term.}
\end{figure}

Once a fully working algorithm has been made for the CartPole, a new, more complex environment will be implemented to improve test and improve the generality of the program. A good next step is to increase the dimensionality of the state space, for example by adding a spring between the players and the base of the cart.

\bibliographystyle{abbrv}
\bibliography{Report}

\begin{figure}[b]
   \centering
   \includegraphics[width=0.9\textwidth]{Figures/CartpoleFlowchart.png}
   \caption{\label{fig:flowchart} A flowchart showing the control flow of the current program. Green boxes depict more complex processes with further explanation in the main text.}
\end{figure}

\end{document}
