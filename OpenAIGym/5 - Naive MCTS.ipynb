{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "''' \n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. \n",
    "The system is controlled by applying a force of +1 or -1 to the cart. \n",
    "The pendulum starts upright, and the goal is to prevent it from falling over. \n",
    "A reward of +1 is provided for every timestep that the pole remains upright.\n",
    "The episode ends when the pole is more than 15 degrees from vertical, or the \n",
    "cart moves more than 2.4 units from the center.\n",
    "'''\n",
    "\n",
    "# obs =  env.reset()    #returns an initial observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01642717  0.01088803 -0.03848342 -0.00444985]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "observation = env.reset()\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "class dotdict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        return self[name]\n",
    "\n",
    "args = dotdict({\n",
    "    'lr': 0.0005,\n",
    "    'dropout': 0.3,\n",
    "    'epochs': 1,\n",
    "    'batch_size': 64, #256,\n",
    "    'pareto': 5000, # a factor to multiply action loss by to get optimal loss (5000 ish seems to work well)\n",
    "    'cuda': torch.cuda.is_available(),\n",
    "    'num_channels': 512,\n",
    "    'goal_steps': 201, #200 is the limit for cart-pole\n",
    "    'score_requirement': 65,\n",
    "    'initial_games': 30000,\n",
    "    'policyUpdates': 2,    #10\n",
    "    'policyEpisodes': 250, #250\n",
    "    'numMCTSSims': 100,\n",
    "})\n",
    "print(args.pareto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialExamples():\n",
    "        allExamples = []\n",
    "        accepted_scores = np.array([])    # just the scores that met our threshold\n",
    "        init_controller = Controller()\n",
    "        init_controller.nnet = None                 # so that executeEpisode doesn't try anything weird\n",
    "\n",
    "        # --------------- ITERATE THROUGH 10000 EPISODE ------------------\n",
    "        for _ in range(args.initial_games):\n",
    "\n",
    "            exampleGame = init_controller.executeEpisode()\n",
    "\n",
    "            # --------- SAVE EXAMPLE (EPISODE) IF (SCORE > THRESHOLD) ----------\n",
    "            # Note, it does not save the score! Therefore all episodes with score > threshold\n",
    "            # are treated equally (not the best way of doing this!)\n",
    "            if exampleGame[0, 5] >= args.score_requirement:\n",
    "\n",
    "                accepted_scores = np.append(accepted_scores, exampleGame[0, 5])\n",
    "\n",
    "                if len(allExamples) == 0:\n",
    "                    allExamples = exampleGame\n",
    "                else:\n",
    "                    allExamples = np.vstack(   (allExamples, exampleGame)   )\n",
    "\n",
    "\n",
    "        # -------- PRINT STATS ------------\n",
    "        avg_mean, avg_median = np.mean(accepted_scores), np.median(accepted_scores)\n",
    "        print('Average accepted score: ', avg_mean)\n",
    "        print('Median score for accepted scores: ', avg_median)\n",
    "        print(Counter(accepted_scores))\n",
    "        print(len(accepted_scores))\n",
    "\n",
    "        return allExamples, avg_mean, avg_median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controller (coach) Class: PI and Episode Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller():\n",
    "    \n",
    "    def __init__(self, nnet = None, mcts = None):\n",
    "        self.nnet = nnet   # the nnet \"is part of\" the controller -> composition (or aggregation?.. implemented by pointer/reference in c++)\n",
    "        self.mcts = mcts\n",
    "                 \n",
    "    def policyIteration(self):\n",
    "        scores = np.array([])\n",
    "\n",
    "        #self.nnet = Net() # don't actually need to initiate the \"prev_nnet\", since it is defined when we create a controller object\n",
    "        init_examples, curr_mean, curr_median = initialExamples()  # Don't need to pass a model\n",
    "        a_loss, v_loss, batch_acc = self.nnet.train_model(examples = init_examples)\n",
    "\n",
    "        for i in range(args.policyUpdates):\n",
    "\n",
    "            # ----- GENERATE A BATCH OF EPISODES BASED ON THE PREVIOUS NET----------\n",
    "            exampleBatch = []\n",
    "            for e in range(args.policyEpisodes):\n",
    "                self.mcts = MCTS(self.nnet)     # reset search tree for each episode here (rather than the episode)\n",
    "                example = self.executeEpisode() # dont need to pass self.mcts into execute episode as it is an attribute\n",
    "                scores = np.append(scores, example[0, 5])\n",
    "\n",
    "                if len(exampleBatch) == 0:\n",
    "                    exampleBatch = example\n",
    "                else:\n",
    "                    exampleBatch = np.vstack(   (exampleBatch, example)   )\n",
    "\n",
    "\n",
    "            # -------- CREATE CHALLENGER POLICY BASED ON EXAMPLES GENERATED BY PREVIOUS POLICY -------------------\n",
    "            new_nnet = Net() # create a new net to train\n",
    "            a_loss, v_loss, batch_acc = new_nnet.train_model(examples = exampleBatch)\n",
    "\n",
    "\n",
    "            # -------- PRINT STATS ON NEW POLICY -------------\n",
    "            new_mean, new_median = np.mean(scores), np.median(scores)\n",
    "            print('Average accepted score: ', new_mean)\n",
    "            print('Median score for accepted scores: ', new_median)\n",
    "            print(Counter(scores))\n",
    "            print(\"Current Policy: \", curr_mean, curr_median)\n",
    "\n",
    "            # ---------- COMPARE AND UPDATE POLICIES --------------\n",
    "            if new_mean >= curr_mean and new_median >= curr_median:\n",
    "                self.nnet = new_nnet\n",
    "                curr_mean, curr_median = new_mean, new_median\n",
    "                print(\"Policy Updated!\")\n",
    "                print(\"New Policy: \", curr_mean, curr_median)\n",
    "\n",
    "        return self.nnet\n",
    "    \n",
    "    def executeEpisode(self):\n",
    "        ''' Generate and example episode of [4 x observation(t), action(t), E[return(t)]]. \n",
    "            All values are in a (n x 6) numpy array where n is the number of steps for the \n",
    "            episode to finish or the limit of 200 steps. The MCTS is reset before every \n",
    "            episode in PI.'''\n",
    "                 \n",
    "        score = 0\n",
    "        example = np.zeros( (args.goal_steps, 6) )\n",
    "        observation = env.reset(); done = False; # list of 4 elements\n",
    "                 \n",
    "        # --------- ITERATE UP TO 500 STEPS PER EPISODE -------------\n",
    "        for t in range(args.goal_steps):\n",
    "\n",
    "            # --------- GENERATE ACTION ------------\n",
    "            # We can generate random actions or actions from a MCTS with the current policy (nnet)\n",
    "            if self.nnet == None:\n",
    "                action = env.action_space.sample()   # choose random action (0-left or 1-right)\n",
    "            else:\n",
    "                mcts_env = deepcopy(env)            #takes the current state of the env so mcts can play it out from there\n",
    "                action_prob = self.mcts.getActionProb(example[t, :], done, mcts_env) # self.nnet already exists in self.mcts\n",
    "                \n",
    "                #the random sample is generated as if a were np.arange(a)\n",
    "                #p = The probabilities associated with choosing each entry in a\n",
    "                #len(p) = 2 for this -> choosing [0, 1] with probabilities action_prob associated with each\n",
    "                action = np.random.choice(len(action_prob), p=action_prob) \n",
    "                              \n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "\n",
    "            # --------- STORE STATE-ACTION PAIR + SCORE ------------\n",
    "            example[t, 0:4] = observation[0:4]\n",
    "            example[t, 4:6] = [action, score]\n",
    "\n",
    "            observation = np.array(new_observation)\n",
    "            score += reward    # +1 for every frame we haven't fallen\n",
    "\n",
    "            if done: \n",
    "                break\n",
    "\n",
    "        example[:, 5] = score - example[:, 5]    # Convert scores to E[return] \n",
    "        return example[0:int(score), :] # we only want to return the parts with actual values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte-Carlo Tree Search Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-8\n",
    "\n",
    "class MCTS():\n",
    "\n",
    "    def __init__(self, nnet):\n",
    "        self.nnet = nnet    # New policy per policy iteration\n",
    "                            # (technically reinitialised each episode for PI, but it's the same until it's updated)\n",
    "        self.c_puct = 0.1\n",
    "        self.Qsa = {}       # stores Q values for s,a (as defined in the paper)\n",
    "        self.Nsa = {}       # stores #times edge s,a was visited\n",
    "        self.Ns = {}        # stores #times board s was visited\n",
    "        self.Ps = {}        # stores initial policy (returned by neural net)\n",
    "\n",
    "    def getActionProb(self, start_state_action_return, done, m_env):\n",
    "        \"\"\"\n",
    "        This function performs numMCTSSims simulations of MCTS starting from\n",
    "        the state/step that it was called on (i.e. mcts_env).\n",
    "        Returns:\n",
    "            probs: a policy vector where the probability of the ith action is\n",
    "                   proportional to Nsa[(s,a)]**(1./temp)\n",
    "        \"\"\"\n",
    "        # ------------ FILL OUT THE MCTS TREE -------------\n",
    "        for i in range(args.numMCTSSims):\n",
    "            mcts_env = deepcopy(m_env) # possibly have this outside for loop?\n",
    "            self.search(start_state_action_return, done, mcts_env)\n",
    "        \n",
    "        # ------------- AND COUNT THE NUMBER OF TIMES EACH STATE WAS VISITED --------------\n",
    "        #s = self.game.stringRepresentation(canonicalBoard)\n",
    "        counts = [self.Nsa[(s,a)] if (s,a) in self.Nsa else 0 for a in range(0, 2)] # range(0, 2) = action size\n",
    "        \n",
    "        # ----------------- THEN NORMALISE TO A PROBABILITY -------------------\n",
    "        #if temp==0:\n",
    "        #    bestA = np.argmax(counts)\n",
    "        #    probs = [0]*len(counts)\n",
    "        #    probs[bestA]=1\n",
    "        #    return probs\n",
    "        \n",
    "        counts = [x**(1./temp) for x in counts]\n",
    "        probs = [x/float(sum(counts)) for x in counts]\n",
    "        return probs\n",
    "\n",
    "\n",
    "    def search(self, sqd, done, env):\n",
    "        # ---------------- TERMINAL STATE ---------------\n",
    "        if done == True:\n",
    "            return 1\n",
    "\n",
    "        # ------------- EXPLORING FROM A LEAF NODE ----------------------\n",
    "        #check if the state has a policy from it yet, if not then its a leaf\n",
    "        if s not in self.Ps:\n",
    "            self.Ps[s], v = self.nnet.predict(s)\n",
    "\n",
    "            #check if the neural net has assigned a +ve prob to any policy\n",
    "\n",
    "            sum_Ps_s = np.sum(self.Ps[s])\n",
    "            if sum_Ps_s > 0:\n",
    "                self.Ps[s] /= sum_Ps_s    # renormalize\n",
    "            else:\n",
    "                # if they were all zero then they are equally probable: (this shouldn't usually happen)\n",
    "                # NB! All valid moves may = 0 if NNet architecture is insufficient or you've get overfitting or something else.\n",
    "                # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.   \n",
    "                print(\"All valid moves were masked, do workaround.\")\n",
    "                self.Ps[s] = self.Ps[s] + valids\n",
    "                self.Ps[s] /= np.sum(self.Ps[s])\n",
    "\n",
    "            self.Ns[s] = 0\n",
    "            return v\n",
    "\n",
    "\n",
    "        # ------------- GET BEST ACTION -----------------------------\n",
    "        # search through the valid actions and update the UCB for all actions then update best acions\n",
    "        max_u, best_a = -float(\"inf\"), -1\n",
    "        for a in range(1):\n",
    "            if (s,a) in self.Qsa:\n",
    "                u = self.Qsa[(s,a)] + self.cpuct*self.Ps[s][a]*np.sqrt(self.Ns[s])/(1+self.Nsa[(s,a)])\n",
    "            else:\n",
    "                u = self.cpuct*self.Ps[s][a]*math.sqrt(self.Ns[s] + 1e-8)     # Q = 0 ?\n",
    "\n",
    "            if u > max_u:\n",
    "                max_u = u\n",
    "                best_a = a\n",
    "        a = best_a\n",
    "\n",
    "\n",
    "        # ----------- RECURSION TO NEXT STATE ------------------------\n",
    "        sp, reward, done, info = env.step(a)\n",
    "        v = self.search(sp, reward, done)\n",
    "\n",
    "\n",
    "        # ------------ BACKUP Q-VALUES AND N_VISITED -----------------\n",
    "        # after we reach the terminal condition then the stack unwinds and we\n",
    "        # propagate up the tree backing up Q and N as we go\n",
    "        if (s,a) in self.Qsa:\n",
    "            self.Qsa[(s,a)] = (self.Nsa[(s,a)]*self.Qsa[(s,a)] + v)/(self.Nsa[(s,a)]+1)\n",
    "            self.Nsa[(s,a)] += 1\n",
    "\n",
    "        else:\n",
    "            self.Qsa[(s,a)] = v\n",
    "            self.Nsa[(s,a)] = 1\n",
    "\n",
    "        self.Ns[s] += 1\n",
    "        return v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Policy (Neural Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(4, 128)\n",
    "        self.l2 = nn.Linear(128, 256)\n",
    "        self.l3 = nn.Linear(256, 128)\n",
    "        self.l4 = nn.Linear(128, 32)\n",
    "        \n",
    "        self.dp = nn.Dropout(p = args.dropout)  # Suragnair used 0.3\n",
    "        self.a1 = nn.Linear(32, 2)    # want an action vector output: [log(prob right), log(prob left)]\n",
    "        self.v1 = nn.Linear(32, 32)\n",
    "        self.v2 = nn.Linear(32, 1)    # Output the expected return\n",
    "\n",
    "    def forward(self, obs):\n",
    "        #in_size = x.size(0)\n",
    "        x = F.relu(self.dp(self.l1(obs)))\n",
    "        x = F.relu(self.dp(self.l2(x)))\n",
    "        x = F.relu(self.dp(self.l3(x)))\n",
    "        x = F.relu(self.dp(self.l4(x)))\n",
    "        \n",
    "        #x = x.view(in_size, -1)  # flatten the tensor\n",
    "        a = self.a1(self.dp(x))\n",
    "        action_probs = F.log_softmax(a, dim = -1)    # choose the dimension such that we get something like \n",
    "                                                     # [exp(-0.6723) +  exp(-0.7144)] = 1 for the output\n",
    "        v = self.v2(self.dp(self.v1(x)))  # get a linear value for the expected return\n",
    "        return action_probs, v                      \n",
    "    \n",
    "    \n",
    "    def train_model(self, examples):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=args.lr)\n",
    "        action_loss, value_loss, accuracy = [], [], []\n",
    "\n",
    "        # ------------- CONVERT TO CORRECT DATA TYPE ----------------\n",
    "        gpu = torch.device(\"cpu\")\n",
    "        states = torch.tensor(  examples[:, 0:4] ,  dtype = torch.float)       #reshapes into a (23002, 4) array\n",
    "        target_actions = torch.tensor(  examples[:, 4], dtype = torch.long)    #reshapes into a (23002, 2) array \n",
    "        target_returns = torch.tensor(  examples[:, 5],  dtype = torch.float) \n",
    "        \n",
    "\n",
    "        #if args.cuda:  #if we're using the GPU:\n",
    "        #    states, target_actions, target_returns = states.contiguous().cuda(), target_actions.contiguous().cuda(), target_returns.contiguous().cuda()\n",
    "        #states, target_pis, target_vs = Variable(states), Variable(target_actions), Variable(target_returns)\n",
    "        # We should permute data before batching really. (X is a torch Variable)\n",
    "        #permutation = torch.randperm(X.size()[0])\n",
    "        \n",
    "        for epoch in range(args.epochs):\n",
    "            print('EPOCH ::: ' + str(epoch+1))\n",
    "            self.train()     # set module in training mode\n",
    "            batch_idx = 0\n",
    "            \n",
    "            for index in range(0, len(target_returns) - args.batch_size, args.batch_size):        \n",
    "\n",
    "                # -------- GET BATCHES -----------\n",
    "                #indices = permutation[i:i+batch_size]\n",
    "                batch_idx = int(index / args.batch_size) + 1 #add one so stats print properly\n",
    "                batch_states = states[index : index+args.batch_size] # torch.Size([64, 4])\n",
    "                batch_actions = target_actions[index : index+args.batch_size] # torch.Size([64])\n",
    "                batch_returns = target_returns[index: index+args.batch_size] # torch.Size([64])\n",
    "\n",
    "                # -------------------- FEED FORWARD ---------------------- \n",
    "                pred_actions, pred_return = self.forward(batch_states) # torch.Size([64, 2]) and torch.Size([64, 1])\n",
    "                batch_NumWrong = torch.abs(torch.argmax(pred_actions, dim = 1) - batch_actions).sum()\n",
    "            \n",
    "                a_loss = F.nll_loss(pred_actions, batch_actions, reduction = 'elementwise_mean')*args.pareto #standard is \"elementwise_mean\"\n",
    "                \n",
    "                #print(pred_actions.detach(), batch_actions.detach(), a_loss.detach())\n",
    "                \n",
    "                # Suragnair uses tanh for state_values, but their values are E[win] = [-1, 1] where -1 = loss\n",
    "                # Here we are using the length of time that we have been \"up\"\n",
    "                #v_loss = F.binary_cross_entropy(torch.sigmoid(pred_return[:, 0]), torch.sigmoid(batch_returns))\n",
    "                v_loss = F.mse_loss(pred_return[:, 0], batch_returns, reduction = 'elementwise_mean')\n",
    "\n",
    "                action_loss.append(a_loss);    value_loss.append(v_loss)\n",
    "                tot_loss = a_loss + v_loss\n",
    "\n",
    "                # ----------- COMPUTE GRADS AND BACKPROP ----------------\n",
    "                optimizer.zero_grad()\n",
    "                tot_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # --------- PRINT STATS --------------\n",
    "                # Get array of predicted actions and compare with target actions to compute accuracy\n",
    "                \n",
    "                accuracy.append(  1 - (batch_NumWrong.detach().numpy()) / args.batch_size    ) #counts the different ones\n",
    "                if batch_idx % 8 == 0:\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tA-Loss: {:.4f}, V-Loss: {:.4f}\\tAccuracy: {:.5f}'.format(\n",
    "                            epoch+1, \n",
    "                            batch_idx * args.batch_size, \n",
    "                            states.size()[0],\n",
    "                            100 * batch_idx * args.batch_size / states.size()[0], \n",
    "                            a_loss,\n",
    "                            v_loss,\n",
    "                            accuracy[batch_idx - 1])\n",
    "\n",
    "                     )\n",
    "\n",
    "        return action_loss, value_loss, accuracy # removed self?\n",
    "    \n",
    "    def test(self, render = False):\n",
    "        self.eval()\n",
    "        scores, expected_scores, choices = [], np.zeros(args.goal_steps), []\n",
    "\n",
    "        # ------- PLAY SOME TEST GAMES ----------\n",
    "        for each_game in range(10):\n",
    "            env.reset()\n",
    "            score, E_score = 0, []\n",
    "            game_memory, prev_obs = [], []\n",
    "\n",
    "            for _ in range(args.goal_steps):    # play up to (200) frames\n",
    "                if render:\n",
    "                    env.render()\n",
    "\n",
    "                # ----- GENERATE AN ACTION -------\n",
    "                if len(prev_obs)==0:    # start by taking a random action\n",
    "                    action = env.action_space.sample()   \n",
    "\n",
    "                else:                   # After that take the best predicted action by the neural net\n",
    "                    x = torch.tensor(   prev_obs,   dtype = torch.float    )\n",
    "                    action_prob, e_score = self.forward(x)\n",
    "                    action = np.argmax(   action_prob.detach().numpy()   )\n",
    "                    E_score.append(   e_score.detach().numpy()   )  # see how the game updates it expected score as we move through\n",
    "\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                prev_obs = new_observation\n",
    "\n",
    "                # ----- RECORD RESULTS -------\n",
    "                choices.append(action)   # just so we can work out the ratio of what we're predicting\n",
    "\n",
    "                game_memory.append([new_observation, action])\n",
    "                score += reward\n",
    "                if done: break\n",
    "\n",
    "            scores.append(score)    # Record the score of each game\n",
    "            padding = np.zeros(int(args.goal_steps - score + 1), dtype = int)\n",
    "            E_score = np.append([np.array(E_score)], [padding])\n",
    "            expected_scores = np.vstack((expected_scores, E_score))\n",
    "\n",
    "        print('Average Score:',sum(scores)/len(scores))\n",
    "        print('choice 1 (right): {:.4f}  choice 0 (left): {:.4f}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices)))\n",
    "        print(Counter(scores))\n",
    "\n",
    "        x = np.linspace(1, len(expected_scores[0]), num = len(expected_scores[0]))\n",
    "        plt.plot(x, expected_scores[1])\n",
    "        plt.plot(x, expected_scores[3])\n",
    "        plt.xlabel(\"Steps taken\"); plt.ylabel(\"Expected Return (steps until failure)\")\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accepted score:  75.58227848101266\n",
      "Median score for accepted scores:  72.0\n",
      "Counter({66.0: 32, 68.0: 28, 65.0: 27, 67.0: 22, 71.0: 18, 72.0: 15, 70.0: 15, 73.0: 14, 69.0: 13, 75.0: 10, 76.0: 9, 74.0: 9, 77.0: 9, 78.0: 8, 82.0: 7, 79.0: 6, 88.0: 6, 84.0: 5, 90.0: 5, 85.0: 5, 80.0: 4, 89.0: 4, 86.0: 4, 87.0: 4, 83.0: 3, 91.0: 3, 101.0: 3, 94.0: 3, 95.0: 3, 93.0: 3, 96.0: 2, 92.0: 2, 81.0: 2, 109.0: 2, 120.0: 1, 124.0: 1, 107.0: 1, 100.0: 1, 131.0: 1, 115.0: 1, 110.0: 1, 97.0: 1, 111.0: 1, 102.0: 1, 112.0: 1})\n",
      "316\n",
      "EPOCH ::: 1\n",
      "Train Epoch: 1 [512/23884 (2%)]\tA-Loss: 3477.8572, V-Loss: 1669.7456\tAccuracy: 0.48438\n",
      "Train Epoch: 1 [1024/23884 (4%)]\tA-Loss: 3417.7891, V-Loss: 2203.4231\tAccuracy: 0.59375\n",
      "Train Epoch: 1 [1536/23884 (6%)]\tA-Loss: 3440.0027, V-Loss: 1435.6803\tAccuracy: 0.54688\n",
      "Train Epoch: 1 [2048/23884 (9%)]\tA-Loss: 3398.5415, V-Loss: 1836.2518\tAccuracy: 0.54688\n",
      "Train Epoch: 1 [2560/23884 (11%)]\tA-Loss: 3397.7146, V-Loss: 2090.0366\tAccuracy: 0.56250\n",
      "Train Epoch: 1 [3072/23884 (13%)]\tA-Loss: 3338.2798, V-Loss: 6031.8555\tAccuracy: 0.64062\n",
      "Train Epoch: 1 [3584/23884 (15%)]\tA-Loss: 3360.2419, V-Loss: 1828.4868\tAccuracy: 0.59375\n",
      "Train Epoch: 1 [4096/23884 (17%)]\tA-Loss: 3394.8308, V-Loss: 2509.9807\tAccuracy: 0.50000\n",
      "Train Epoch: 1 [4608/23884 (19%)]\tA-Loss: 3367.5005, V-Loss: 2134.0127\tAccuracy: 0.57812\n",
      "Train Epoch: 1 [5120/23884 (21%)]\tA-Loss: 3346.6992, V-Loss: 1361.5322\tAccuracy: 0.57812\n",
      "Train Epoch: 1 [5632/23884 (24%)]\tA-Loss: 3538.4407, V-Loss: 1340.1431\tAccuracy: 0.51562\n",
      "Train Epoch: 1 [6144/23884 (26%)]\tA-Loss: 3208.7590, V-Loss: 2132.6682\tAccuracy: 0.62500\n",
      "Train Epoch: 1 [6656/23884 (28%)]\tA-Loss: 3470.0825, V-Loss: 1448.2949\tAccuracy: 0.53125\n",
      "Train Epoch: 1 [7168/23884 (30%)]\tA-Loss: 3385.3833, V-Loss: 1939.7965\tAccuracy: 0.64062\n",
      "Train Epoch: 1 [7680/23884 (32%)]\tA-Loss: 3717.7605, V-Loss: 1580.1145\tAccuracy: 0.56250\n",
      "Train Epoch: 1 [8192/23884 (34%)]\tA-Loss: 3366.3223, V-Loss: 1250.0297\tAccuracy: 0.56250\n",
      "Train Epoch: 1 [8704/23884 (36%)]\tA-Loss: 3356.8423, V-Loss: 946.4295\tAccuracy: 0.59375\n",
      "Train Epoch: 1 [9216/23884 (39%)]\tA-Loss: 3831.3242, V-Loss: 535.6627\tAccuracy: 0.42188\n",
      "Train Epoch: 1 [9728/23884 (41%)]\tA-Loss: 3501.9749, V-Loss: 814.5046\tAccuracy: 0.59375\n",
      "Train Epoch: 1 [10240/23884 (43%)]\tA-Loss: 3557.0537, V-Loss: 773.3280\tAccuracy: 0.54688\n",
      "Train Epoch: 1 [10752/23884 (45%)]\tA-Loss: 3218.6057, V-Loss: 621.6374\tAccuracy: 0.65625\n",
      "Train Epoch: 1 [11264/23884 (47%)]\tA-Loss: 3648.9001, V-Loss: 417.2668\tAccuracy: 0.45312\n",
      "Train Epoch: 1 [11776/23884 (49%)]\tA-Loss: 3560.6975, V-Loss: 560.2481\tAccuracy: 0.48438\n",
      "Train Epoch: 1 [12288/23884 (51%)]\tA-Loss: 3307.0122, V-Loss: 1444.1038\tAccuracy: 0.68750\n",
      "Train Epoch: 1 [12800/23884 (54%)]\tA-Loss: 3349.9363, V-Loss: 680.1829\tAccuracy: 0.59375\n",
      "Train Epoch: 1 [13312/23884 (56%)]\tA-Loss: 3449.8850, V-Loss: 495.3731\tAccuracy: 0.50000\n",
      "Train Epoch: 1 [13824/23884 (58%)]\tA-Loss: 3411.6868, V-Loss: 746.1184\tAccuracy: 0.59375\n",
      "Train Epoch: 1 [14336/23884 (60%)]\tA-Loss: 3468.0310, V-Loss: 670.8714\tAccuracy: 0.51562\n",
      "Train Epoch: 1 [14848/23884 (62%)]\tA-Loss: 3416.6445, V-Loss: 653.3054\tAccuracy: 0.56250\n",
      "Train Epoch: 1 [15360/23884 (64%)]\tA-Loss: 3223.0942, V-Loss: 499.8328\tAccuracy: 0.64062\n",
      "Train Epoch: 1 [15872/23884 (66%)]\tA-Loss: 3346.2615, V-Loss: 812.5511\tAccuracy: 0.59375\n",
      "Train Epoch: 1 [16384/23884 (69%)]\tA-Loss: 3504.8159, V-Loss: 362.8109\tAccuracy: 0.53125\n",
      "Train Epoch: 1 [16896/23884 (71%)]\tA-Loss: 3297.7874, V-Loss: 357.6162\tAccuracy: 0.59375\n",
      "Train Epoch: 1 [17408/23884 (73%)]\tA-Loss: 3257.9163, V-Loss: 936.0513\tAccuracy: 0.64062\n",
      "Train Epoch: 1 [17920/23884 (75%)]\tA-Loss: 3327.7664, V-Loss: 976.7140\tAccuracy: 0.57812\n",
      "Train Epoch: 1 [18432/23884 (77%)]\tA-Loss: 3399.5276, V-Loss: 457.2278\tAccuracy: 0.65625\n",
      "Train Epoch: 1 [18944/23884 (79%)]\tA-Loss: 3544.4746, V-Loss: 482.4843\tAccuracy: 0.50000\n",
      "Train Epoch: 1 [19456/23884 (81%)]\tA-Loss: 3412.0408, V-Loss: 359.8445\tAccuracy: 0.56250\n",
      "Train Epoch: 1 [19968/23884 (84%)]\tA-Loss: 3589.1506, V-Loss: 756.2588\tAccuracy: 0.53125\n",
      "Train Epoch: 1 [20480/23884 (86%)]\tA-Loss: 3376.4011, V-Loss: 597.9254\tAccuracy: 0.62500\n",
      "Train Epoch: 1 [20992/23884 (88%)]\tA-Loss: 3569.3796, V-Loss: 991.8469\tAccuracy: 0.53125\n",
      "Train Epoch: 1 [21504/23884 (90%)]\tA-Loss: 3232.1460, V-Loss: 189.4369\tAccuracy: 0.59375\n",
      "Train Epoch: 1 [22016/23884 (92%)]\tA-Loss: 3236.5774, V-Loss: 303.6053\tAccuracy: 0.62500\n",
      "Train Epoch: 1 [22528/23884 (94%)]\tA-Loss: 3427.4307, V-Loss: 274.4075\tAccuracy: 0.53125\n",
      "Train Epoch: 1 [23040/23884 (96%)]\tA-Loss: 3521.1665, V-Loss: 355.2526\tAccuracy: 0.60938\n",
      "Train Epoch: 1 [23552/23884 (99%)]\tA-Loss: 3353.2231, V-Loss: 351.5661\tAccuracy: 0.59375\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-773f8deb11a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mController\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicyIteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-0e9ed24ab102>\u001b[0m in \u001b[0;36mpolicyIteration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicyEpisodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmcts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMCTS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnet\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# reset search tree for each episode here (rather than the episode)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                 \u001b[0mexample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuteEpisode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# dont need to pass self.mcts into execute episode as it is an attribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m                 \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-0e9ed24ab102>\u001b[0m in \u001b[0;36mexecuteEpisode\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[0mmcts_env\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m            \u001b[1;31m#takes the current state of the env so mcts can play it out from there\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[0maction_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmcts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetActionProb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmcts_env\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# self.nnet already exists in self.mcts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;31m#the random sample is generated as if a were np.arange(a)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-29dbfacfd354>\u001b[0m in \u001b[0;36mgetActionProb\u001b[1;34m(self, start_state_action_return, done, m_env)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumMCTSSims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mmcts_env\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm_env\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# possibly have this outside for loop?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_state_action_return\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmcts_env\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# ------------- AND COUNT THE NUMBER OF TIMES EACH STATE WAS VISITED --------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-29dbfacfd354>\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, sqd, done, env)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# ------------- EXPLORING FROM A LEAF NODE ----------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m#check if the state has a policy from it yet, if not then its a leaf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "test = Controller(Net())\n",
    "best_model = test.policyIteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score: 169.6\n",
      "choice 1 (right): 0.5077  choice 0 (left): 0.4923\n",
      "Counter({200.0: 4, 131.0: 2, 145.0: 1, 150.0: 1, 154.0: 1, 185.0: 1})\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model.test(render = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
