\documentclass[../main.tex]{subfiles}
\begin{document}
\onlyinsubfile{\tableofcontents{}}

\chapter{Results and Discussion}

\section{Training}

In this section, the training of the player and adversary will be investigated. The first sections will discuss the results of a baseline neural network, $f_{\theta}(\boldsymbol{x})$, which takes the true state as input. This is followed by a comparison with those of the network that took the 2-dimensional state as input, $f_{\theta}(\boldsymbol{x}^{(2D)})$. The results are interpreted with a discussion of the merits and shortcomings of the techniques. For the training runs shown, the parameters used are given in \Cref{tab:tparams} and \Cref{sec:architectures}.

\begin{table}[h]
    \centering
    \begin{tabular}{c | c}
        Parameter & Value \\
        \hline
        Unopposed Episodes for initial iteration &  100\\
        Opposed Episodes per iteration &  40\\
        Number of MCTS Simulations per step & 20 \\
        $F_2^{(max)}$  &  0.05$F_1$
    \end{tabular}
    \caption{Training parameters used}
    \label{tab:tparams}
\end{table}

Iterations start at zero and are defined by two main steps: (1) A batch of training examples are generated using the current neural networks. (2) The neural networks are retrained using these examples. The full algorithm can be found in \cref{alg:PI}.

\subsection{MCTS}
\label{sec:dismcts}

\Cref{fig:tree} shows the structure of a typical tree search without neural network training. This is significantly better than with no tree search, which typically achieves an episode length of 14.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Tree.PNG}
    \caption{An example MCTS tree structure lasting 33 steps with 15 MCTS simulations per step and both agents are untrained. The adversary has an average power of 5\% of the player ($F_2 = 0.05F_1$). Lighter (yellow) colours represent ``better'' states.}
    \label{fig:tree}
    \vspace{0.7cm}
    \centering
    \includegraphics[width=\textwidth]{MicroTree.PNG}
    \caption{A zoomed in view of \Cref{fig:tree}. The three colours of each node are based on the value of $U=Q(\boldsymbol{x}, u) + ucb$ as in \Cref{equ:IPucb}, the number of state-action visits, $N(\boldsymbol{x}, u)$, and the state-cost, $c(\boldsymbol{x}_t)$. The red nodes follow the true trajectory. \texttt{Ns} ($N(\boldsymbol{x})$) is the total number of state visits, this does not equal the action-state visit count, \texttt{Nsa} ($N(\boldsymbol{x}, u)$), these differ along the true trajectory due to the re-running of the MCTS simulation.}
    \label{fig:microtree}
\end{figure}

\Cref{fig:microtree} shows the sequential maximisation that occurs during the tree search. Note that the action-value, $Q(\boldsymbol{x}_t, u_t)$, for the adversary is negated and $Q$ is between -1 and 0 for the player. $N(\boldsymbol{x}, u)$ is correlated with the predicted value, $V_{pred}$, and $U$, which suggests that the neural network and tree search are working as expected. At the first branching in \Cref{fig:microtree} the $ucb$ is 17\% of $U$. Intuitively, this is within the correct range as a larger exploration term would cause the MCTS to branch more, diminishing the computational benefit of the structured search, whereas a smaller exploration term inhibits learning. To achieve this ratio, a value of 1 was chosen for $c_{explore}$. The action-state visit counts are 22 and 17, which are markedly higher than the number of simulations per step (10 in this example), suggesting the MCTS is performing a highly structured search. Furthermore, the similar predicted state values, $v_\theta(\boldsymbol{x})$, reflect similar visit counts of each branch.

\subsection{State and Action Predictions}

In this section, the output of the baseline neural network for a specific episode (\Cref{fig:value,fig:action2,fig:action4}) is investigated. From this, the suitability of using a neural network for both the adversary and the player, what the neural networks are modelling and limitations of the model are discussed.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Training1D/value_vs_step.PNG}
    \caption{The predicted state-values (from both the adversarial and player networks) plotted against the true state-value for a number of iterations.}
    \label{fig:value}
\end{figure}

\Cref{fig:value} shows that by iteration 4 the player and adversary networks both follow the true value very closely, and by  iteration 10 the mean squared error (MSE) is almost zero (\Cref{fig:nnetlosses10}). This is likely due to the adversary pushing only in one direction, therefore making the value very predictable. In iteration 2, the adversary values are more ``jagged'', alternating between a positive and negative evaluation depending on whether the player (which roughly alternates between pushing left and right when near the equilibrium) has pushed the pendulum further over or not. I.e. by iteration 10, the adversary has learnt to predict the player's actions, which suggests that the prediction of the state-value is not impeded by the use of two neural networks.

\Cref{fig:action2,fig:action4} show the change in policy action predictions between iteration 2 and iteration 4. By iteration 2 the adversary has already decided on the direction that it will push, however the player is still tuning itself. At iteration 4, the player 4 switches between pushing left and right at a lower frequency.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{Training1D/action_vs_step2.PNG}
    \caption{MCTS and policy predicted actions vs step for an episode in iteration 2.}
    \label{fig:action2}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{Training1D/action_vs_step4.PNG}
    \caption{MCTS and policy predicted actions vs step for an episode in iteration 4.}
    \label{fig:action4}
\end{figure}

The output of the predicted values for a small slice of $\dot{x}$ is shown in \cref{fig:3D4}. Along all pairs of axes, the data is not linearly separable. Furthermore principal component analysis (PCA) shows that the predicted values are not linearly separable along the principal axes either (\cref{fig:PCA}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Training1D/3D4.PNG}
    \caption{A 3D plot of policy-predicted actions during training. The central ``blob'' is projected onto the $(x,\dot{\theta})$, $(x, \theta)$ and $(\theta, \dot{\theta})$ axes.}
    \label{fig:3D4}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Training1D/PCA.PNG}
    \caption{Principal Component Analysis of the data in \cref{fig:3D4}.}
    \label{fig:PCA}
\end{figure}

In the optimal control solution, a real valued force would be used to get the pendulum to the equilibrium. Due to the constraint over the actions, the policy must approximate that by repeatedly pushing left/right in order to get an average force that is equivalent to a continuous one. By iteration 4 (\cref{fig:action4}), the neural network matches the MCTS prediction at almost every step and switches quickly between pushing left and right. This implies that a trajectory through the $(x, \theta)$ plane would look like a ``wave'', where the power cycle of the wave in the vicinity of the point is proportional to the instantaneous average power predicted at that point. Additionally, a higher frequency of the wave is related to a faster response time of the system. Note that higher frequencies encode information more densely, and therefore, require deeper neural networks. Portions of this wave are shown in the $(x, \theta)$ projection of \cref{fig:3D4}.

The instantaneous predicted average power of the policy can be estimated by applying a low-pass filter to the predicted action probabilities (orange line in \cref{fig:action2,fig:action4}). The low-pass filter has a normalised cut-off frequency of $\frac{1/\tau}{2/\delta t}$, where $\tau$ is characteristic time of the system and $\frac{2}{\delta t}$ is the Nyquist frequency. For this system, however, this only provides marginal insight as, for this implementation, the time step is almost equal to the characteristic time. Decreasing the time step requires the episode length to be increase such that the real-world episode time is constant at $dt\times S = 0.01s \times 400steps = 4s$. Therefore, decreasing $dt$ causes a significant increase computational time.

\Cref{fig:action2,fig:action4} show that the frequency decreases between iteration 2 and iteration 4; therefore, the neural network is not making use of the highest frequency that it has the capacity to use as in iteration 2. This suggests that against this adversary, a frequency higher than the natural frequency is not necessary for good control. This may be because a higher frequency is needed for more uncertain situations and, since the adversary is only pushing in one direction, does not need to oscillate as quickly.

\subsection{Power Matching and the Policy}

In this section, the problem of ``power matching'' the player and adversary are discussed. Power matching involves balancing the effect of each agents actions such that they have an equal effect on the system. The efficacy of pre-training the player as a solution to this are analysed. This is followed by a discussion on why the player and adversary act as they do, with an interpretation of the policies they have learnt.

It was found that training the player unopposed indefinitely will cause the player to balance the pendulum within a few time steps from a wide range starting positions, however introducing an adversary at or close to ``full power'' (i.e. $F_2  \gtrapprox  0.0476F_1$, \cref{sec:choiceofaction}) will cause the inverted pendulum to fall immediately. Therefore, an unopposed training episode and an incremental increase in adversary power were introduced. After implementing this, typically, either the adversary is too strong and the episode is over within 20 time steps, or the adversary learns to simply push in one direction only. \Cref{fig:actionbias} shows this transition occuring over just one episode. Since the adversary was not trained on iteration 0, iteration 1 has MCTS predictions to push both right and left. However, during the training in iteration 2, even with approximately $10\%$ of training examples pushing left, the network trains to push right 100\% of the time, and as a result the MCTS also predicts that pushing right is always the best action.

The player does not suffer from this bias and has a far more uniform spread of policy predictions in iteration 2. In iteration 3, the policy predicts either left or right with 100\% conviction, however, these are distributed in a roughly 50:50 split, with the proportion of $p(Player \; Action = left) > 0.5$ being slightly higher, to counter the adversary always pushing right. As a result of the adversary being trained to push solely in one direction, the proportion states that are visited are biased in one direction. Visually, this is as if the system is performing the set-point tracking of a reference angle slightly off centre, and therefore slowly moves in one direction until reaching $x_{max}$.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Training1D/actionbias.PNG}
    \caption{Histograms showing the distribution of predicted actions by the MCTS and pure policy for both the player and adversary.}
    \label{fig:actionbias}
    \vspace{0.7cm}
    \centering
    \includegraphics[width=\textwidth]{Training1D/statebias.PNG}
    \caption{Histograms showing the proportion of time that the system was in each state on the second iteration.}
    \label{fig:statebias}
\end{figure}

The most likely cause for this bias is the differing abilities between agents due to variations in the pre-training of the player and the power curve of the adversary. Factors such as how much to train the player before introducing an adversary and how strong to make the adversary are difficult to balance. The player can be better trained by increasing the number of training examples it has from unopposed episodes and by increasing the number of unopposed policy iterations. Additionally, the adversary's power, and power curve, can be adjusted by varying $\gamma$ and $F_2$ in the adversary handicap formula, $F_2(1-\gamma^{\, i-N+1})$. However, by training the player more and under-powering the adversary, the adversary learns to push solely in one direction. Pre-training the player too little typically leads to the overtraining of the neural network on a small number of examples, which is difficult to re-train against. Alternatively, under-powering the adversary, or making it too weak for too long seems to cause the adversary to learn that slowly pushing the inverted pendulum to $x_{max}$ is the best option, rather than attempting to push it over quickly.

The cost function being weighted as $\boldsymbol{w}^T = [0.4, 0.1, 0.7, 1]$ may have exacerbated this behaviour. A low weight the on $x$-position compared to the angular position may mean that the player is not prioritising stopping the slow drift to $\boldsymbol{x}_{max}$. Yet from the adversary's perspective, since it is initially learning whilst underpowered, this is the best way to reduce the state value. A possible way in which this could have been prevented would have been to set the $x$-position cost to zero. In this case, if the adversary solely pushes in one direction, either the player would be able to find an equilibrium by leaning into the adversary's force - minimising the $\dot{x}$ cost, but outputting a constant $\theta$ cost; or the player would attempt to minimise the $\theta$ cost and as a result move in one direction continuously - outputting a constant $\dot{x}$ cost. Both of these scenarios do not consistently improve the adversary's state value, therefore this could force the adversary to attempt to knock the pendulum over, rather than slowly improving the state-value.


\subsection{Comparison with $f_{\theta}(\boldsymbol{x}^{(2D)})$}

The training of the convolutional neural network (CNN) results in training episodes with very oscillatory behaviour compared to the the baseline network (\Cref{fig:motion1D,fig:motion2D}).

\begin{figure*}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Training1D/motion_history_2.PNG}
        \caption{True-State Input}
        \label{fig:motion1D}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Training2D/motion_history_2.PNG}
        \caption{2-Dimensional State Input}
        \label{fig:motion2D}
    \end{subfigure}
    \caption{Motion history images showing the trajectories of 5 episodes for both neural networks. Higher valued colours represent more recent positions (or positions with multiple visits as the images are computed as a sum of discounted states).}
\end{figure*}

The oscillatory nature of these episodes are due to the player switching between pushing left and right much slower than the characteristic time of the system (\cref{fig:2Daction4}).

This is likely due to the a relatively shallow neural network being used (2 convolutional layers followed by 1 linear layer. See \cref{sec:architectures}). A deeper neural network was found to exceed the memory allocation limit of the GPU used. It is possible that this caused a more simple model of the controller to be trained, such that the data became easily linearly separable, as shown in \cref{fig:2D3D4} where the $(x, \theta)$ and $(\theta, \dot{\theta})$ axes have easily separable predictions. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Training2D/action_vs_step4.PNG}
    \caption{MCTS and policy predicted actions vs step for an episode in iteration 4.}
    \label{fig:2Daction4}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Training2D/3D4.PNG}
    \caption{A 3D plot of policy-predicted actions after the 4th iteration of training of the CNN.}
    \label{fig:2D3D4}
\end{figure}

As the training iterations increased, the oscillations became larger, until iteration 9 where they became large enough that the controller could not stabilise the pendulum at all.

Another possibility for the poor training of the CNN is the relatively small number of training examples used. Each iteration generates less than 16,000 examples (40 episodes each with 400 steps), and at each iteration the last 5 iteration's examples are used for training, which gives a maximum of 80,000 training examples - or 40,000 for each agent. With 40 bins there are 40x40=1600 possible positions. The number of possible 2D-states increases as $1600^n$, where n is the number time steps recorded, therefore, even after 2 time steps, there are 64 times more possible 2D-states than training examples.

\newpage
\section{Evaluation and Testing}

In this section, the results of experiments pitting the player against a number of adversaries is discussed and the performance and robustness of the algorithm is evaluated.

\subsection{Performance Against a Random Opponent}

Against a random adversary with $F_2 = 0.05F_1$, the player's performance improves with the number of MCTS simulations when it has not been trained. However, the performance after training is constant, as shown in \cref{fig:rand}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Training1D/RandomLength.PNG}
    \includegraphics[width=\textwidth]{Training1D/RandomScore.PNG}
    \caption{The average episode length and score for each iteration over 50 episodes. The standard deviation of episode lengths is also shown as a lighter region around the means. A random adversary is used with $F_2=0.05F_1$.}
    \label{fig:rand}
\end{figure}

\Cref{fig:rand} shows the first iteration's score and length increase with the number of MCTS simulations per step, as the neural network is trained without an adversary. However, since the player has a model of what it expects the ``best'' adversary to do incorporated into its MCTS, if the player expects the adversary to push in only one direction then it will perform poorly against an adversary that does not follow that policy.

The \textit{ucb} (exploration) term in \cref{equ:IPucb}
is only $\sim 20\%$ of the Q value when the player is untrained (\cref{sec:dismcts}). If the player is trained such that its model adversary is predicting $p(left) = 0.01$ then the magnitude of $ucb_{left}$ would need to be $\frac{0.2}{0.01} = 20$ times greater than $ucb_{right}$ to get an exploration term of a similar magnitude. For the number of MCTS simulations per step used in testing, the exploration term never becomes large enough to match the size of the exploitation (Q) term. Therefore, the player acts greedily with respect to its model agent's policy predictions. Due to this, the MCTS trees from iteration 2 onwards do not search along trajectories in which the agent gives a small action probability i.e. the tree search never expands left if the adversary always predicts right. This means that if the random adversary picks left then the MCTS must effectively reset the tree, and furthermore, the value predicted for that branch is not representative of the branch as a whole. Therefore, the performance of the player is defined by the average length of sequential right pushes rather than the depth of the MCTS. This is why the mean score and episode lengths are roughly equal after the adversary is trained and is a major weakness for the players trained against an adversary.

Note that the episode length and score are highly correlated. This is to be expected as the player acts in a similar way for each episode. However, the score cannot necessarily be predicted from the episode length, for example, if an agent that allows oscillatory behaviour was implemented, such as the CNN, $f(\boldsymbol{x^{(2D)}})$, it's score will be low, but the average length of an episode will not necessarily be low.

\subsection{Performance Against Variable $F_2$ and Trained Adversaries}

When the player is pitted against a trained adversary, either the adversary pushes in the same direction as the player's model of the adversary, or it does not. In the former case, the player performs very well lasting to 350 steps on average, whereas in the latter case it performs particularly badly, often falling in less than 20 steps.

When the trained player is pitted against random adversaries with different strengths, the episode length follows a predictable pattern of falling over faster when there is a higher force applied by the adversary. Note, that in order for the player to be able to control for adversaries with varying forces, the player's model of the adversary must also apply that force, otherwise the tree search would not be able to look ahead. This is one of the main drawbacks of this method; there must be perfect knowledge of the adversary and the system must be deterministic.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Training1D/VariousLength.PNG}
    \caption{The average episode length for each iteration over 50 episodes. A selection of adversary forces, $F_2$, were used. 15 MCTS simulations per step was used for every run.}
    \label{fig:rand}
\end{figure}

\chapter{Conclusions And Future Work}

\section{Conclusions}

The aim of this project was to investigate the application of the AlphaZero algorithm to control problems, specifically the control of dynamical systems. The inverted pendulum was chosen as a simple starting dynamical system which also met the requirements of being sufficiently complex such that a trivial solution was not possible.

In order to adapt AlphaZero into a control algorithm a state cost function and a type of receding horizon for the state value were designed. The state value is calculated retrospectively and used for training the neural network. Adding these allowed for the algorithm to deal with continuous states and indefinite horizons with no easily discernable ``winner'', unlike the board games played by AlphaZero. In addition to this, two different methods of state representation were considered: the true state, $\boldsymbol{x}^T=[x, \; \dot{x}, \; \theta, \; \dot{\theta}]$, and a 2-dimensional (2D) state, $\boldsymbol{x}^{(2D)}$. The true state had the benefit of being continuous with a low number of dimensions, which made it more ideal for this specific problem. However, the 2D state did not rely on measurements of the system velocities. Relying purely on positional information allows for a more generalisable state space representation as in real systems, states are often unobservable. Furthermore, the binning of the states was proven to be lossless in the limit of infinite time, assuming a deterministic model and that the neural network could emulate an optimal Kalman Filter.

The policy was represented as two neural networks, paired at each iteration. It was found that the two networks predicted similar values for each state suggesting that the use of a network for each the player and adversary did not hinder learning.

Balancing the effect of each agents actions such that they have an equal effect on the system, or ``power matching'' was one of the more difficult problems faced. The adversary was chosen to act at the tip of the pendulum so that the adversary would be forced to learn a different policy than the player. However, due to the effect of gravity and a different point of action, the adversary and player had unequal effects on the system. It was found that simply choosing the force of the adversary to cause the same angular acceleration of the pendulum ($F_2 = 0.05F_1$) would lead to the pendulum falling very quickly, meaning that minimal training examples were recorded; or, if a smaller force was chosen, the adversary was too weak to push over the pendulum. In order to counter this, the adversary force was incrementally increased. In the case of the adversary being too weak, the adversary invariably learnt to push solely in one direction, it was also found that pre-training did not improve this. This was postulated to be the most efficient way for the adversary to maximise the cost of the system. Due to the adversary learning this, robustness to general disturbances was severely hindered. It was found that the player's internal model of the adversary could not deal with more general adversaries.

Against a random adversary, the performance of the player was found to be independent of the depth of the Monte-Carlo tree search, and was defined by the average length of sequential right pushes. This was due to the resetting of the search when the adversary pushed in a direction unexpected by the player.

Due to computational limitations, the episode length and time step were limited such that the time step was only marginally faster than the characteristic time of the system. In an ideal simulation, the time step would be many times smaller than the characteristic time, allowing for the controller to approximate continuous actions (pulse width modulation). It was found, even with the limited depth of the neural networks used, that the policies could model rapid changes in output with respect to the input space well. This suggests that there is potential for this method to work well on many problems. 

The potential benefits of successfully adapting AlphaZero were: robust disturbance handling, the ability to control non-linear systems, and a wide range of possible applications. It has been shown that MCTS can be used to improve the control of a system to great effect, and there is no limitation for this due to system non-linearities. This method can model any system given that it is deterministic and the player has a perfect knowledge of the action space of the adversary. However, more work is needed for this to be considered ``robust control''.

\section{Recommended Future Work}

This project has shown that the application of AlphaZero to control problems is feasible, however, there are a number of areas in which further work needs to be done.

The simplest improvement for this project is to use more computing power. The neural networks and number of training examples in this project were limited primarily due to the specifications of the GPU used. Therefore, a more powerful computer would allow data to be collected more quickly, and deeper neural networks to be designed.

The most pressing area of interest is the problem of ``power matching''. In the discussion, it was postulated that the reason for the adversary learning to push solely in one direction was due to the choice of cost function. It has been shown in other control schemes for the inverted pendulum that the adversary should be able to push the pendulum over without overpowering the player \cite{invpen}. This can be done by pushing at the system's resonant frequency, which takes advantage of inherent lag in the system. Therefore, a simple improvement may be to introduce a first order lag, $F_t = \lambda F_{k-1} + (1-\lambda)u_k$.

Recently, Deepmind released a blog post on a new algorithm, \textit{AlphaStar}, which defeated the world champion in StarCraft II \cite{alphastar}. Due to the complexity of StarCraft, the algorithm was designed to keep a record of different adversarial strategies, and was deemed ``good'' if it could defeat all of them. It is speculated that there are multiple strategies to push over the IP, one being to slowly push it to one side, and another being to take advantage of mistakes made by the controller (player). Making a record of multiple adversaries may provide a method to overcome this.

\onlyinsubfile{\subfile{Bibliography.tex}}
\onlyinsubfile{\appendix}
\onlyinsubfile{\subfile{Appendix.tex}}

\end{document}