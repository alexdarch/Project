\documentclass[../main.tex]{subfiles}
\begin{document}
\chapter{Appendices}
\section{Inverted Pendulum Dynamics Derivation}
\label{appendix:invpen}

The state space equations for the Inverted Pendulum can be found using d'Alembert forces. Define the distance and velocity vectors to the important points:

\begin{align*}
    & \boldsymbol{r}_P = x \boldsymbol{i} \\
    & \boldsymbol{r}_{B_1/P} = L sin \theta \boldsymbol{i} + L cos \theta \boldsymbol{j} \\
    & \boldsymbol{r}_{B_1} = (x+L cos \theta)\boldsymbol{i}+L \dot{\theta} sin \theta  \boldsymbol{j} \\
    & \dot{\boldsymbol{r}}_{B_1} = (\dot{x} + L\dot{\theta}cos\theta)\boldsymbol{i} - L\dot{\theta}sin\theta \boldsymbol{j}
\end{align*}

Linear Momentum, $\boldsymbol{\rho} = \sum_i m_i \dot{\boldsymbol{r}}_{i/o} = m \dot{\boldsymbol{r}}_{B_1} + M \dot{\boldsymbol{r}}_{P}$:

\begin{equation*}
\boldsymbol{\rho} = 
\begin{bmatrix} (M+m)\dot{x} + m L\dot{\theta}cos{\theta} \\ -m L\dot{\theta}sin{\theta} \\ 0 \end{bmatrix} 
\end{equation*}

Moment of momentum about P, $\boldsymbol{h}_P = \boldsymbol{r}_{B_1/P} \times m \boldsymbol{\dot{r}}_{B_1}$:

\begin{align*}
\boldsymbol{h}_P & = -m L(L\dot{\theta} + \dot{x}cos\theta) \boldsymbol{k} \\
\therefore \boldsymbol{\dot{h}}_P & = -mL(L\ddot{\theta} + \ddot{x}cos\theta - \dot{x}\dot{\theta}sin\theta) \boldsymbol{k}
\end{align*}

Balance moments using $\boldsymbol{\dot{h}}_P + \boldsymbol{\dot{r}}_P \times \boldsymbol{\rho} = \boldsymbol{Q}_{e}$ and $ \boldsymbol{Q}_{e} = \boldsymbol{r}_{B_1/P} \times -mg\boldsymbol{j} + \boldsymbol{r}_{B_2/P} \times F_2 \boldsymbol{i}$:
\begin{equation*}
\boldsymbol{\dot{h}}_P + \boldsymbol{\dot{r}}_P \times \boldsymbol{\rho} = 
\begin{bmatrix} 0 \\ 0 \\ -m L (\ddot{x} cos\theta + L\ddot{\theta}) \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ -L(m g sin\theta + 2 F_2 cos \theta) \end{bmatrix} = \boldsymbol{Q}_e
\end{equation*}

And also balance linear momentum using $\boldsymbol{F}_e = \dot{\boldsymbol{\rho}}$:

\begin{equation*}
    \dot{\boldsymbol{\rho}} = \begin{bmatrix} (m+M)\ddot{x} + m L(\ddot{\theta}cos\theta - \dot{\theta}^2 sin\theta) \\ -m L(\ddot{\theta}sin\theta + \dot{\theta}^2 cos\theta) \\ 0 \end{bmatrix}
    = \begin{bmatrix} F_1 + F_2 \\ R-(M+m)g \\ 0 \end{bmatrix} = \boldsymbol{F}_e
\end{equation*}

Finally write the system dynamics in terms of $\ddot{\theta}$ and $\ddot{x}$:

\begin{align}
\ddot{\theta}\big(M+m sin^2\theta \big)L & = \bigg(\frac{2M+m}{m}F_2-F_1\bigg)cos\theta + g(M+m)sin\theta - mL\dot{\theta}^2 sin\theta cos\theta\\
\ddot{x}(M+m sin^2\theta) & = F_1 - F_2cos(2\theta)+ m sin\theta(L\dot{\theta}^2-g cos\theta)
\end{align}

Simplifying by substituting in constants, the full state space equation are:

\begin{equation}
\begin{bmatrix} \dot{x} \\ \ddot{x} \\ \dot{\theta} \\ \ddot{\theta} \end{bmatrix}  =
\begin{bmatrix} \dot{x} \\ \frac{\big(\frac{2M+m}{m}F_2-F_1\big)cos\theta + g(M+m)sin\theta - mL\dot{\theta}^2 sin\theta cos\theta}{(M + m sin^2\theta)} \\ \dot{\theta} \\ \frac{F_1 - F_2cos(2\theta)+ msin\theta(L\dot{\theta}^2-g cos\theta)}{L(M+m sin^2\theta)} \end{bmatrix} =
\begin{bmatrix} f_1(\boldsymbol{x}, F_1, F_2) \\ f_2(\boldsymbol{x}, F_1, F_2) \\ f_3(\boldsymbol{x}, F_1, F_2) \\ f_4(\boldsymbol{x}, F_1, F_2) \end{bmatrix}
\end{equation}

Using Lyapunov's indirect method, the linearised equations about the equilibrium, $\boldsymbol{x}_e = [x_e, \dot{x}_e, \theta_e, \dot{\theta}_e]^T = [0, 0, 0, 0]^T$, are:
\begin{equation}
    \newcommand{\pf}[2]{\frac{\partial f_#1}{\partial #2}\big{|}_{\boldsymbol{x}_e}}
\begin{bmatrix} \delta \dot{x} \\ \delta \ddot{x} \\ \delta \dot{\theta} \\ \delta \ddot{\theta} \end{bmatrix} 
=   \begin{bmatrix} 
    \pf{1}{x} & \pf{1}{\dot{x}} & \pf{1}{\theta} & \pf{1}{\dot{\theta}} \\
    \pf{2}{x} & \pf{2}{\dot{x}} & \pf{2}{\theta} & \pf{2}{\dot{\theta}} \\
    \pf{3}{x} & \pf{3}{\dot{x}} & \pf{3}{\theta} & \pf{3}{\dot{\theta}} \\
    \pf{4}{x} & \pf{4}{\dot{x}} & \pf{4}{\theta} & \pf{4}{\dot{\theta}} \\
    \end{bmatrix}
    \begin{bmatrix} \delta x \\ \delta \dot{x} \\ \delta \theta \\ \delta \dot{\theta} \end{bmatrix}
+  \begin{bmatrix} \pf{1}{F_1} & \pf{1}{F_2} \\ \pf{2}{F_1} & \pf{2}{F_2} \\ \pf{3}{F_1} & \pf{3}{F_2} \\ \pf{4}{F_1} & \pf{4}{F_2} \end{bmatrix} 
\begin{bmatrix} \delta F_1 \\ \delta F_2 \end{bmatrix}
\end{equation}

\begin{equation}
    \label{equ:linearisedA}
    \newcommand{\pf}[2]{\frac{\partial f_#1}{\partial #2}\big{|}_{\boldsymbol{x}_e}}
\begin{bmatrix} \delta \dot{x} \\ \delta \ddot{x} \\ \delta \dot{\theta} \\ \delta \ddot{\theta} \end{bmatrix} 
=   \begin{bmatrix} 
    0 & 1 & 0 & 0 \\
    0 & 0 & \frac{mg}{M} & 0 \\
    0 & 0 & 0 & 1 \\
    0 & 0 & \frac{(m+M)}{ML}g & 0 \\
    \end{bmatrix}
    \begin{bmatrix} \delta x \\ \delta \dot{x} \\ \delta \theta \\ \delta \dot{\theta} \end{bmatrix}
+  \begin{bmatrix} 0 & 0 \\ \frac{1}{M} & -\frac{1}{M} \\ 0 & 0 \\ -\frac{1}{ML} & \frac{2M+m}{mML} \end{bmatrix} 
\begin{bmatrix} \delta F_1 \\ \delta F_2 \end{bmatrix}
\end{equation}

The eigenvalues are given by $det(\lambda I - A) = \lambda^2 (\lambda^2 - \frac{(m+M)}{ML}g) = 0$. Therefore, the system is unstable about $\boldsymbol{x}_e$ due to the right half plane pole, $\lambda = \sqrt{\frac{(m+M)}{ML}g}$. Additionally, the time constant of this unstable system is $\tau = \sqrt{\frac{ML}{g(m+M)}}$. Note, if $M >> m, \tau \rightarrow \sqrt{\frac{L}{g}}$, which is the time constant for a simple pendulum.

It can be proved that the inverted pendulum system is controllable by showing:

\begin{equation}
    rank[\boldsymbol{B} \; \boldsymbol{AB} \; \boldsymbol{A^2B} \; \boldsymbol{A^3B}] = 4
\end{equation}
Therefore for any initial condition we can reach $\boldsymbol{x}_e$ in finite time under these linear assumptions.

\section{Propagation of Quantisation Error}{
\label{appendix:quant}
{
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\bu}{\boldsymbol{u}}

The state space model for the quantisation of the linearised inverted pendulum can be written as:
\begin{align}
    \label{equ:stateequsapp}
&\bx_t^{(2D)} = C\bx_t + \boldsymbol{V}_t \hspace{3cm} \boldsymbol{V}_t \sim \mathcal{U} \left(\begin{bmatrix} \frac{1}{\delta x} \\ \frac{1}{\delta \theta} \end{bmatrix} \right) \\
    &\bx_t = A \bx_{t-1} + B\bu_t
\end{align}

Where A and B are the linearised system dynamics (valid for small time steps), and C is the linear transformation to a 2D state space, with quantisation noise \textbf{V}.

Assuming the quantisation bin sizes, $\delta x$ and $\delta \theta$, are small and that $x$ and $\theta$ are independent within the bin, the quantisation noise can be modelled as uniform random variables with covariance, $cov(\boldsymbol{V}, \boldsymbol{V}) = \mathbb{E}[\boldsymbol{V}\boldsymbol{V}^T]$:

\begin{equation}
= \mathbb{E} \begin{bmatrix} x^2 & x \theta \\ \theta x & \theta^2 \end{bmatrix} 
= \begin{bmatrix} \int^{\delta x/2}_{-\delta x/2} x^2 \cdot \frac{1}{\delta x} dx & 0 \\ 0 & \int^{\delta \theta/2}_{-\delta \theta/2} \theta^2 \cdot \frac{1}{\delta \theta} d\theta \end{bmatrix} 
= \begin{bmatrix} \frac{\delta x^2}{12} & 0 \\ 0 & \frac{\delta \theta^2}{12} \end{bmatrix}
\end{equation}

For simplicity, let $\delta x = \delta \theta$, and therefore, $cov(\boldsymbol{V}, \boldsymbol{V}) = \sigma_v^2 I$.


Kalman filtering can be used to find an optimal estimate $\hat{\bx}_n = K[\bx_n | \boldsymbol{y}_{n-k:n}]$ using the algorithm (derived in \cite{4f7}).

\begin{algorithm}[h]
    \caption{Multivariate Kalman Filtering}
    \label{alg:kalman}
    \begin{algorithmic}[1]
        \State \textbf{Given:} $\hat{\bx}_n = K[\bx_n | \boldsymbol{y}_{1:n}]$ and $\Sigma_n = \mathbb{E}[(\bx_n - \hat{\bx}_n)(\bx_n - \hat{\bx}_n)^T]$
       \Statex
       \Statex \textbf{Prediction:}
       \State $\bar{\bx}_{n+1} = K[\bx_n | \boldsymbol{y}_{1:n}] = A \hat{\bx}_n + B \bu_{n+1}$
       \State $\bar{\Sigma}_{n+1} = \mathbb{E}[(\bx_{n+1} - \bar{\bx}_{n+1})(\bx_{n+1} - \bar{\bx}_{n+1})^T] = A \Sigma_n A^T + \Sigma_w$
       \Comment{$\Sigma_w = \boldsymbol{0}$}
       \Statex
       \Statex \textbf{Update:}
       \State $\tilde{\boldsymbol{y}}_{n+1} = \boldsymbol{y}_{n+1} - C\bar{\bx}_{n+1}$
       \Comment{Calculate Innovation residual}
       \State $S_{n+1} = \Sigma_v + C \bar{\Sigma}_{n+1} C^T$
       \Comment{Calculate Innovation Covariance}
       \State $\hat{\bx}_{n+1} = \bar{\bx}_{n+1} + \bar{\Sigma}_{n+1}C^T S_{n+1}^{-1} \tilde{\boldsymbol{y}}_{n+1}$
       \Comment{$\Sigma_v = \sigma_v^2 I$}
       \State $\Sigma_{n+1} = (I - \bar{\Sigma}_{n+1}C^T S_{n+1}^{-1} C)\bar{\Sigma}_{n+1}$
    \end{algorithmic}
 \end{algorithm}

 Thus we can find the optimal linear estimate of the covariance of \Cref{equ:stateequsapp} from \cref{alg:kalman} line 7:

 \begin{align}
    \Sigma_{n+1} &= (I - \bar{\Sigma}_{n+1}C^T S_{n+1}^{-1} C)\bar{\Sigma}_{n+1} \\
    &= (I - \bar{\Sigma}_{n+1}C^T (\sigma_v^2 I + C \bar{\Sigma}_{n+1} C^T)^{-1} C)\bar{\Sigma}_{n+1}, \hspace{1cm} where \;\; \bar{\Sigma}_{n+1} = A \Sigma_n A^T
 \end{align}

 For the univariate case this reduces to:

 \begin{align}
    \sigma_{n+1}^2 &= \bar{\sigma}_{n+1}^2 (1 - \frac{C^2 \bar{\sigma}_{n+1}^2}{\sigma_v^2 + C^2 \bar{\sigma}_{n+1}^2}) \\
    &= A^2\sigma_n^2(1 - \frac{C^2 A^2\sigma_n^2}{\sigma_v^2 + C^2 A^2 \sigma_n^2})
 \end{align}
}

It is obvious from this that as $\sigma_v^2 \rightarrow 0, \; \sigma_{n+1}^2 \rightarrow 0$. Furthermore, if the spectral radius, $\rho \left(A^2(1 - \frac{C^2 A^2\sigma_n^2}{\sigma_v^2 + C^2 A^2 \sigma_n^2}) \right) < 1$, then the mean squared error from quantisation will reduce to zero. Since $0 < ||\frac{C^2 A^2\sigma_n^2}{\sigma_v^2 + C^2 A^2 \sigma_n^2}||^2_2 < 1$ if $\sigma_v^2 > 0$, a sufficient condition is that $A^2 \leq 1 \implies \rho(A) \leq 1$.

Using Gelfand's Theorem, $\rho(M_1 ... M_n) \leq \rho(M_1)...\rho(M_n)$, and the eigenvalue identity, $(M+cI)v = (c+\lambda)v$. The MSE of the multivariate case can be reduced to:

\begin{align}
1\;  > \; & \rho\left((I - \bar{\Sigma}_{n+1}C^T (\sigma_v^2 I + C \bar{\Sigma}_{n+1} C^T)^{-1} C)\bar{\Sigma}_{n+1}\right) \\
> \; & \rho(\bar{\Sigma}_{n+1})\left[ 1 - \rho(\bar{\Sigma}_{n+1}C^T (\sigma_v^2 I + C \bar{\Sigma}_{n+1} C^T)^{-1} C)  \right] 
\end{align}

Therefore, if $\rho\left(\bar{\Sigma}_{n+1}C^T (\sigma_v^2 I + C \bar{\Sigma}_{n+1} C^T)^{-1} C)\right)$ is less than one, and $\rho(\bar{\Sigma}_{n+1}) < 1$, then this is true. This is equivalent to showing that if $0 < \kappa(\Sigma_{n})\kappa(A)^2 \kappa(CC^T) \leq 1$ and $\rho(AA^T) < 1$, where $\kappa(\cdot)$ denotes the condition number, then the MSE will decay:

\begin{align*}
  \rho\left(\bar{\Sigma}_{n+1}C^T (\sigma_v^2 I + C \bar{\Sigma}_{n+1} C^T)^{-1} C)\right) 
    \leq \; & \rho(\bar{\Sigma}_{n+1})\rho(CC^T) \rho((\sigma_v^2 I + C \bar{\Sigma}_{n+1} C^T)^{-1}) \\
    \leq \; & \rho(\bar{\Sigma}_{n+1})\rho(C C^T) \frac{1}{|\lambda_{min}(\sigma_v^2 I + C \bar{\Sigma}_{n+1} C^T)|} \\
    = \; & \rho(\bar{\Sigma}_{n+1})\rho(CC^T) \frac{1}{|\lambda_{min}(C \bar{\Sigma}_{n+1} C^T)| + \sigma_v^2} \\
    < \; & \frac{|\lambda_{max}(\bar{\Sigma}_{n+1})\lambda_{max}(CC^T)|}{|\lambda_{min}(C \bar{\Sigma}_{n+1} C^T)|} \\
    < \; & \frac{|\lambda_{max}(\bar{\Sigma}_{n+1})\lambda_{max}(CC^T)|}{|\lambda_{min}(\bar{\Sigma}_{n+1}CC^T)|} \\
    < \; & \frac{|\lambda_{max}(\bar{\Sigma}_{n+1})\lambda_{max}(CC^T)|}{|\lambda_{min}(\bar{\Sigma}_{n+1})\lambda_{min}(CC^T)|} \\
    = \; &  |\kappa(\Sigma_{n})|\kappa(A)^2 \kappa(CC^T)
\end{align*}

where the above uses the results $\lambda_i(X\Sigma X^*) = \lambda_i(\Sigma X^*X)$, where $\Sigma$ is symmetric (this equality also holds for spectral radii) and $\lambda_{min}(\bar{\Sigma}_{n+1} C C^T) > \lambda_{min}(\bar{\Sigma}_{n+1})\lambda_{min}(C C^T)$. Note that $\kappa(\Sigma_n)=1$. Therefore sufficient conditions for the decay of the covariance are:
\begin{align}
    (1)& \hspace{1cm} \rho(AA^T) \leq 1 \implies |\lambda_{max}(AA^T)| \leq 1 \\
    (2)& \hspace{1cm} \kappa(A)^2 \leq 1 \implies |\lambda_{min}(A)| = |\lambda_{max}(A)| \\
    (3)& \hspace{1cm} \kappa(CC^T) \leq 1
\end{align}

For the linearised dynamics derived in \cref{equ:linearisedA} the eigenvalues of $AA^T$, $\lambda$, are given by $\lambda^2(1-\lambda)^2 = 0$, and the eigenvalues of A are $\lambda= (0, \,0, \; \pm\sqrt{\frac{(m+M)}{ML}g})$. Therefore (1) and (2) are true.

Therefore, the covariance decays to zero if $\rho(CC^T) \leq 1$ in the linearised multivariate case and the 2D state becomes a lossless estimate of the state. Given that C is a 2x4 projection matrix of the form 
\begin{equation}
C = \begin{bmatrix} c_{11} & 0 & 0 & 0 \\ 0 & 0 & c_{23} & 0 \end{bmatrix} \implies CC^T = \begin{bmatrix} c_{11}^2 & 0 \\ 0 & c_{23}^2 \end{bmatrix}
\end{equation}

By the choice of $\boldsymbol{x}^{(2D)}$ as square, $c_{11} = c_{23}$. By the same logic as above, $\kappa(CC^T) = 1$.


The rate of decay for the univariate case can be determined with a first order approximation about $\sigma_v^2 = 0$:

\begin{align}
    \sigma_{n+1}^2(\sigma_v^2) &= A^2\sigma_n^2(1 - \frac{C^2 A^2\sigma_n^2}{\sigma_v^2 + C^2 A^2 \sigma_n^2}) \\
    &\approx \sigma_{n+1}^2(0) + \delta \sigma_v^2 \frac{\partial \sigma_{n+1}^2(\sigma_v^2)}{\partial \sigma_v^2}|_{\sigma_v^2 = 0} \\
    &= \frac{\delta \sigma_v^2}{(AC\sigma_n)^2}
\end{align}

\section{Neural Network Losses}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{Training1D/nnetloss0.PNG}
    \caption{The losses for both the player and adversary neural networks after one set of training examples.}
    \label{fig:nnetlosses0}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{Training1D/nnetloss1.PNG}
    \caption{The losses for both the player and adversary neural networks after two sets of training examples.}
    \label{fig:nnetlosses1}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{Training1D/nnetloss2.PNG}
    \caption{The losses for both the player and adversary neural networks after three sets of training examples.}
    \label{fig:nnetlosses2}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{Training1D/nnetloss10.PNG}
    \caption{The losses for both the player and adversary neural networks after ten sets of training examples.}
    \label{fig:nnetlosses10}
\end{figure}

\section{Retrospective Risk Assessment}

Being a computer-based project, the risks are minimal. In the original risk assessment the states risks were eye-strain from looking at a computer screen for too long, and back pain from sitting in a single position for too long. Neither of these risks occured over the course of the project.

} % end of bx, bu definitions
\onlyinsubfile{\subfile{Bibliography.tex}}
\end{document}