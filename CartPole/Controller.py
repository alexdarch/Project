from collections import deque
# from Arena import Arena
from MCTS import MCTS
import numpy as np
# from pytorch_classification.utils import Bar, AverageMeter
# import time, os, sys
# from pickle import Pickler, Unpickler
from random import shuffle
from collections import Counter
from Policy import NeuralNet
from copy import deepcopy


class Controller():
    """
    This class executes the self-play + learning. It uses the functions defined
    in Game and NeuralNet. args are specified in main.py.
    """

    def __init__(self, game, nnet, args):
        self.env = game
        self.nnet = nnet
        # self.pnet = self.nnet.__class__(self.env)  # the competitor network
        self.args = args
        self.mcts = MCTS(self.env, self.nnet, self.args)
        self.trainExamplesHistory = []  # history of examples from args.numItersForTrainExamplesHistory latest iterations
        self.skipFirstSelfPlay = False  # can be overriden in loadTrainExamples()

    def executeEpisode(self):
        """
        This function executes one episode of self-play, starting with player 1.
        As the game is played, each turn is added as a training example to
        trainExamples. The game is played till the game ends. After the game
        ends, the outcome of the game is used to assign values to each example
        in trainExamples.
        It uses a temp=1 if episodeStep < tempThreshold, and thereafter
        uses temp=0.
        Returns:
            trainExamples: a list of examples of the form (canonicalBoard, pi, v)
                           pi is the MCTS informed policy vector, v is +1 if
                           the player eventually won the game, else -1.
        """
        example = []
        observation = self.env.reset()
        episodeStep = 0
        state2D = self.env.get2Dstate(observation)
        score = 0
        print("---- NEW EPISODE -------------")
        while True:
            episodeStep += 1
            state2D = self.env.get2Dstate(observation, state2D)

            # ---------- GET PROBABILITIES FOR EACH ACTION --------------
            temp = int(episodeStep < self.args.tempThreshold)  # temperature = 0 if first step, 1 otherwise
            curr_env = deepcopy(self.env)
            print("------ GET ACTION PROB ------------")
            pi = self.mcts.getActionProb(state2D, curr_env, temp=temp)
            print("------ ACTION PROB: ", pi)
            curr_env.printState(state2D)

            example.append([state2D, pi, score])

            # ---------- TAKE NEXT STEP PROBABILISTICALLY ---------------
            action = np.random.choice(len(pi), p=pi)  # take a random choice with pi probability associated with each
            observation, reward, done, info = self.env.step(action)
            score += reward

            if done:
                example = [(x[0], x[1], score-x[2]) for x in example]  # Convert scores to E[return]
                return example  # return this example episode


    def policyIteration(self):
        """
        Performs numIters iterations with numEps episodes of self-play in each
        iteration. After every iteration, it retrains neural network with
        examples in trainExamples (which has a maximium length of maxlenofQueue).
        It then pits the new neural network against the old one and accepts it
        only if it wins >= updateThreshold fraction of games.
        """

        for i in range(1, self.args.numIters + 1):
            # bookkeeping
            print('-----------------ITER ' + str(i) + '--------------------')
            # examples of the iteration
            if not self.skipFirstSelfPlay or i > 1:
                iterationTrainExamples = deque([], maxlen=self.args.maxlenOfQueue)
                scores = np.array([])

                # ----- GENERATE A NEW BATCH OF EPISODES BASED ON THE CURRENT NET----------
                for eps in range(self.args.numEps):
                    self.mcts = MCTS(self.env, self.nnet, self.args)  # reset search tree
                    iterationTrainExamples += self.executeEpisode()


                # save the iteration examples to the history
                self.trainExamplesHistory.append(iterationTrainExamples)

            # -------- CREATE CHALLENGER POLICY BASED ON EXAMPLES GENERATED BY PREVIOUS POLICY -------------------
            # shuffle examples before training
            trainExamples = []
            for e in self.trainExamplesHistory:
                trainExamples.extend(e)
            shuffle(trainExamples)
            new_nnet = NeuralNet(self.env, self.args)  # create a new net to train
            new_nnet.train_model(examples=trainExamples)

            # -------- PRINT STATS ON NEW POLICY -------------
            new_mean, new_median = np.mean(scores), np.median(scores)
            print('Average accepted score: ', new_mean)
            print('Median score for accepted scores: ', new_median)
            print(Counter(scores))
            print("Current Policy: ", curr_mean, curr_median)

            # ---------- COMPARE AND UPDATE POLICIES --------------
            if new_mean >= curr_mean and new_median >= curr_median:
                self.nnet = new_nnet
                curr_mean, curr_median = new_mean, new_median
                print("Policy Updated!")
                print("New Policy: ", curr_mean, curr_median)

            # if len(self.trainExamplesHistory) > self.args.numItersForTrainExamplesHistory:
            #     print("len(trainExamplesHistory) =", len(self.trainExamplesHistory),
            #           " => remove the oldest trainExamples")
            #     self.trainExamplesHistory.pop(0)
            # # backup history to a file
            # # NB! the examples were collected using the model from the previous iteration, so (i-1)
            # self.saveTrainExamples(i - 1)

            # # training new network, keeping a copy of the old one
            # self.nnet.save_checkpoint(folder=self.args.checkpoint, filename='temp.pth.tar')
            # self.pnet.load_checkpoint(folder=self.args.checkpoint, filename='temp.pth.tar')
            # pmcts = MCTS(self.game, self.pnet, self.args)
            #
            # self.nnet.train(trainExamples)
            # nmcts = MCTS(self.game, self.nnet, self.args)
            #
            # print('PITTING AGAINST PREVIOUS VERSION')
            # arena = Arena(lambda x: np.argmax(pmcts.getActionProb(x, temp=0)),
            #               lambda x: np.argmax(nmcts.getActionProb(x, temp=0)), self.game)
            # pwins, nwins, draws = arena.playGames(self.args.arenaCompare)
            #
            # print('NEW/PREV WINS : %d / %d ; DRAWS : %d' % (nwins, pwins, draws))
            # if pwins + nwins > 0 and float(nwins) / (pwins + nwins) < self.args.updateThreshold:
            #     print('REJECTING NEW MODEL')
            #     self.nnet.load_checkpoint(folder=self.args.checkpoint, filename='temp.pth.tar')
            # else:
            #     print('ACCEPTING NEW MODEL')
            #     self.nnet.save_checkpoint(folder=self.args.checkpoint, filename=self.getCheckpointFile(i))
            #     self.nnet.save_checkpoint(folder=self.args.checkpoint, filename='best.pth.tar')
